{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esgryaznova/analyzing-hatespeech-SNA-2022/blob/main/SVD_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0cc9c29f",
      "metadata": {
        "id": "0cc9c29f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2e631142",
      "metadata": {
        "id": "2e631142"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1960ae9a",
      "metadata": {
        "id": "1960ae9a"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "vD7su6eaU3be"
      },
      "id": "vD7su6eaU3be",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7e64223e",
      "metadata": {
        "id": "7e64223e"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"gpt_rubert_ruxl_gpt3_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info('Инициализировали логгер')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyzZICBh3jk-",
        "outputId": "c2ca3f16-04d7-4228-ca4b-144c59235845"
      },
      "id": "LyzZICBh3jk-",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-06-22 09:25:01,635 : INFO : Инициализировали логгер\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('num train samples %d', data.shape[0])\n",
        "data.target_group.value_counts(normalize=True)\n",
        "#сильный дисбаланс классов"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4w3onuu3dDM",
        "outputId": "c918c2b4-cd80-4677-ae18-d3b89584ff95"
      },
      "id": "T4w3onuu3dDM",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-06-22 09:25:01,648 : INFO : num train samples 10382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "man           0.236563\n",
              "women         0.195049\n",
              "other         0.184647\n",
              "born_place    0.175978\n",
              "lgbt          0.113658\n",
              "migrants      0.051146\n",
              "child         0.040551\n",
              "migrant       0.002408\n",
              "Name: target_group, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ef5bb7c7",
      "metadata": {
        "id": "ef5bb7c7"
      },
      "outputs": [],
      "source": [
        "#data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "758e5293",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "758e5293",
        "outputId": "4cd7f2f7-c402-4ab2-9a67-a9e2672b609f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10382, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "235d747f",
      "metadata": {
        "id": "235d747f"
      },
      "outputs": [],
      "source": [
        "# create a list of our conditions\n",
        "conditions = [\n",
        "    (data['Результат 1'] == False) & (data['Результат 2'] == False),\n",
        "    (data['Результат 1'] == False) & (data['Результат 2'] == True),\n",
        "    (data['Результат 1'] == True) & (data['Результат 2'] == False),\n",
        "    (data['Результат 1'] == True) & (data['Результат 2'] == True),\n",
        "    ]\n",
        "\n",
        "# create a list of the values we want to assign for each condition\n",
        "values = ['0', '1', '2', '3']\n",
        "\n",
        "# create a new column and use np.select to assign values to it using our lists as arguments\n",
        "data['respond'] = np.select(conditions, values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.title(\"Toxic responses\", fontdict = {'fontsize': 20})\n",
        "ax = sns.countplot(x = \"target_group\", hue = 'respond', data = data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "l9ioUmbl38X4",
        "outputId": "77a57b85-e2cf-42f5-a573-32b62d9c12b7"
      },
      "id": "l9ioUmbl38X4",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "329fe8f2",
      "metadata": {
        "id": "329fe8f2"
      },
      "outputs": [],
      "source": [
        "data = data[[\"Реплика 2 toxicity\", \"target_group\", \"respond\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6c8396ad",
      "metadata": {
        "id": "6c8396ad"
      },
      "outputs": [],
      "source": [
        "data = data.rename(columns={\"Реплика 2 toxicity\": \"source\", \"target_group\": \"target\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5d3b8aaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d3b8aaf",
        "outputId": "78f80d7d-5c25-45f8-d487-3c6a0b0ec65e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "man           2456\n",
              "women         2025\n",
              "other         1917\n",
              "born_place    1827\n",
              "lgbt          1180\n",
              "migrants       531\n",
              "child          421\n",
              "migrant         25\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "data['target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1222c407",
      "metadata": {
        "id": "1222c407"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "25fbc483",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "25fbc483",
        "outputId": "a04e03a4-f5e9-471d-9c68-dc611099f684"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              source target respond\n",
              "0            слышишь блядь заткни свой ебальник сука  women       3\n",
              "1   я высказал предположение что ... но естествен...  women       3\n",
              "2  блядь сука пиздец вот повезло ебаный рот за чт...  women       1\n",
              "3  в византии были государственные школы. до юсти...  women       1\n",
              "4  лол мы чуть - не проебали сука я убил куры уби...  women       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9aa41715-6c95-4f00-aef5-e36261b525ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "      <th>respond</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>слышишь блядь заткни свой ебальник сука</td>\n",
              "      <td>women</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>я высказал предположение что ... но естествен...</td>\n",
              "      <td>women</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>блядь сука пиздец вот повезло ебаный рот за чт...</td>\n",
              "      <td>women</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>в византии были государственные школы. до юсти...</td>\n",
              "      <td>women</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>лол мы чуть - не проебали сука я убил куры уби...</td>\n",
              "      <td>women</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9aa41715-6c95-4f00-aef5-e36261b525ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9aa41715-6c95-4f00-aef5-e36261b525ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9aa41715-6c95-4f00-aef5-e36261b525ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "0-ZxWeCJUop1"
      },
      "id": "0-ZxWeCJUop1",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2WcZN0kZUxC9"
      },
      "id": "2WcZN0kZUxC9"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xqtTjnmdUotW",
        "outputId": "a2b09c89-30c8-40a2-ce34-64f844ddab95"
      },
      "id": "xqtTjnmdUotW",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source      target respond\n",
              "9962   тебя ебет, чмоньдель? я таких как ты как семе...       women       3\n",
              "1176  ты старый сын суки самый нищий кусок дерьма с ...       women       3\n",
              "551    без американцев эти два клоуна недели не прож...  born_place       3\n",
              "744   ну ты же пиздабол пруфы что я хохол не предост...  born_place       1\n",
              "7012   демиург пидор потому что. да и по его воле же...         man       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8dc0b67c-5d78-4609-9d31-9d6b69500259\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "      <th>respond</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9962</th>\n",
              "      <td>тебя ебет, чмоньдель? я таких как ты как семе...</td>\n",
              "      <td>women</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>ты старый сын суки самый нищий кусок дерьма с ...</td>\n",
              "      <td>women</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>без американцев эти два клоуна недели не прож...</td>\n",
              "      <td>born_place</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>ну ты же пиздабол пруфы что я хохол не предост...</td>\n",
              "      <td>born_place</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7012</th>\n",
              "      <td>демиург пидор потому что. да и по его воле же...</td>\n",
              "      <td>man</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dc0b67c-5d78-4609-9d31-9d6b69500259')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8dc0b67c-5d78-4609-9d31-9d6b69500259 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8dc0b67c-5d78-4609-9d31-9d6b69500259');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "620f8bdf",
      "metadata": {
        "id": "620f8bdf"
      },
      "outputs": [],
      "source": [
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b49850d0",
      "metadata": {
        "id": "b49850d0"
      },
      "outputs": [],
      "source": [
        "g = nx.from_pandas_edgelist(data, source='source', target='target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f5fdeda5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "f5fdeda5",
        "outputId": "b861d986-8b8d-4748-937d-c63ecca76e28"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nx.draw(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "14e2097f",
      "metadata": {
        "id": "14e2097f"
      },
      "outputs": [],
      "source": [
        "edgelist=[]\n",
        "ed_c=g.number_of_edges()\n",
        "for e in g.edges():\n",
        "    node1=e[0]\n",
        "    node2=e[1]\n",
        "    n_c=0\n",
        "    n_c+=len([j for j in g.neighbors(node1)])\n",
        "    n_c+=len([j for j in g.neighbors(node2)])\n",
        "    normalized_count=n_c/ed_c\n",
        "    g[e[0]][e[1]]['weight']=normalized_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "94d3ecfc",
      "metadata": {
        "id": "94d3ecfc"
      },
      "outputs": [],
      "source": [
        "#nx.draw_networkx(g,pos=nx.spring_layout(g))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4551619d",
      "metadata": {
        "id": "4551619d"
      },
      "source": [
        "Дальше загружаем все для сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d5e912b6",
      "metadata": {
        "id": "d5e912b6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import Identity, glorot_uniform, Zeros\n",
        "from tensorflow.keras.layers import Dropout, Input, Layer, Embedding, Reshape,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import networkx as nx\n",
        "import scipy\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "caf89ffc",
      "metadata": {
        "id": "caf89ffc"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(tf.keras.layers.Layer):  # ReLU(AXW)\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation=tf.nn.relu, dropout_rate=0.5,\n",
        "                 use_bias=True, l2_reg=0, \n",
        "                 seed=1024, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.use_bias = use_bias\n",
        "        self.l2_reg = l2_reg\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.activation = tf.keras.layers.Activation(tf.keras.activations.relu)\n",
        "        self.seed = seed\n",
        "       # self.graph=train\n",
        "        \n",
        "    def build(self, input_shapes):\n",
        "        input_dim = int(input_shapes[0][-1])\n",
        "        \n",
        "        self.kernel = self.add_weight(shape=(input_dim,\n",
        "                                             self.units),\n",
        "                                      initializer=glorot_uniform(\n",
        "                                          seed=self.seed),\n",
        "                                      regularizer=l2(self.l2_reg),\n",
        "                                      name='kernel' )\n",
        "        \n",
        "        self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        initializer=Zeros(),\n",
        "                                        name='bias')\n",
        "\n",
        "        self.dropout = Dropout(self.dropout_rate, seed=self.seed)\n",
        "\n",
        "        self.built = True\n",
        "        print('kernel shape',self.kernel.shape)\n",
        "        print('input dimension',input_dim)\n",
        "        \n",
        "    def call(self, inputs, training=None, **kwargs):        \n",
        "        features, A = inputs\n",
        "        A=tf.sparse.to_dense(A)\n",
        "        \n",
        "        v1=tf.matmul(A,features)\n",
        "        \n",
        "        output = tf.matmul(A,self.kernel)\n",
        "        output += self.bias\n",
        "        act = self.activation(output)\n",
        "        \n",
        "        #act._uses_learning_phase = features._uses_learning_phase\n",
        "        return act\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'activation': self.activation,\n",
        "                  'dropout_rate': self.dropout_rate,\n",
        "                  'l2_reg': self.l2_reg,\n",
        "                  'use_bias': self.use_bias,\n",
        "                 \n",
        "                  'seed': self.seed\n",
        "                  }\n",
        "\n",
        "        base_config = super(GraphConvolution, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def GCN(adj_dim,feature_dim,n_hidden, num_class, num_layers=2,activation=tf.nn.relu,dropout_rate=0.5, l2_reg=0 ):\n",
        "    Adj = Input(shape=(None,), sparse=True,name='first')\n",
        "    \n",
        "    X_in = Input(shape=(feature_dim,), sparse=False,name='second')\n",
        "    emb = Embedding(adj_dim, feature_dim,\n",
        "                        embeddings_initializer=Identity(1.0), trainable=False)\n",
        "    X_emb = emb(X_in)\n",
        "#     X_emb=LSTM(3235,return_sequences='True')(X_emb)\n",
        "#     print('Xemb',X_emb)\n",
        "#     H = Reshape([X_emb.shape[-1]])(X_emb)\n",
        "    H=X_emb\n",
        "    print('H shape',H)\n",
        "        \n",
        "#     print(type(Adj))\n",
        "#     H=X_in\n",
        "    for i in range(3):\n",
        "        if i == num_layers - 1:\n",
        "            activation = tf.nn.softmax\n",
        "            n_hidden = num_class\n",
        "        h = GraphConvolution(n_hidden, activation=activation, dropout_rate=dropout_rate, l2_reg=l2_reg)([H,Adj])\n",
        "    output = h\n",
        "    model = Model(inputs=[X_in,Adj], outputs=output)\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "def normalize_adj(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = scipy.sparse.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
        "    else:\n",
        "        d = scipy.sparse.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj).tocsr()\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, symmetric=True):\n",
        "    adj = adj + scipy.sparse.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "label_y= LabelEncoder()\n",
        "labels=label_y.fit_transform(train_data['target'])\n",
        "\n",
        "# y_train=list(set(list(labels)))\n",
        "\n",
        "y_train=encode_onehot(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5858e3c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5858e3c8",
        "outputId": "a3347bc8-83ac-4f65-ef55-dac5d82b8d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H shape KerasTensor(type_spec=TensorSpec(shape=(None, 5599, 5599), dtype=tf.float32, name=None), name='embedding/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding'\")\n",
            "kernel shape (5599, 32)\n",
            "input dimension 5599\n",
            "kernel shape (5599, 8)\n",
            "input dimension 5599\n",
            "kernel shape (5599, 8)\n",
            "input dimension 5599\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " second (InputLayer)            [(None, 5599)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 5599, 5599)   31348801    ['second[0][0]']                 \n",
            "                                                                                                  \n",
            " first (InputLayer)             [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " graph_convolution_2 (GraphConv  (None, 8)           44800       ['embedding[0][0]',              \n",
            " olution)                                                         'first[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,393,601\n",
            "Trainable params: 44,800\n",
            "Non-trainable params: 31,348,801\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "175/175 [==============================] - 11s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.2208\n",
            "Epoch 2/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.6955\n",
            "Epoch 3/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.8930\n",
            "Epoch 4/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 5/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 6/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 7/20\n",
            "175/175 [==============================] - 8s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 8/20\n",
            "175/175 [==============================] - 8s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 9/20\n",
            "175/175 [==============================] - 8s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 10/20\n",
            "175/175 [==============================] - 8s 47ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 11/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 12/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 13/20\n",
            "175/175 [==============================] - 8s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 14/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 15/20\n",
            "175/175 [==============================] - 7s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9028\n",
            "Epoch 16/20\n",
            "175/175 [==============================] - 8s 44ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9028\n",
            "Epoch 17/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9028\n",
            "Epoch 18/20\n",
            "175/175 [==============================] - 7s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9030\n",
            "Epoch 19/20\n",
            "175/175 [==============================] - 8s 43ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9032\n",
            "Epoch 20/20\n",
            "175/175 [==============================] - 8s 42ms/step - loss: nan - categorical_crossentropy: nan - acc: 0.9028\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbed1149650>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "g=nx.from_pandas_edgelist(train_data, source='source',target='target')\n",
        "A=nx.adjacency_matrix(g) #,nodelist=range(g.number_of_nodes()))\n",
        "A=preprocess_adj(A)\n",
        "feature_dim = A.shape[-1]\n",
        "X = np.arange(A.shape[-1])\n",
        "X_n=[]\n",
        "for i in range(feature_dim):\n",
        "    X_n.append(X)\n",
        "X=np.asarray(X_n)\n",
        "# print('X',X)\n",
        "model_input = [X, A]\n",
        "# print('X',type(X))\n",
        "# print('A',type(A))\n",
        "\n",
        "# print('feature_dim',feature_dim)\n",
        "model = GCN(A.shape[-1],feature_dim, 32, y_train.shape[-1],  dropout_rate=0.5, l2_reg=2.5e-4 )\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                  weighted_metrics=['categorical_crossentropy', 'acc'])\n",
        "# print(type(model_input[0]),type(model_input[1]))\n",
        "\n",
        "model.fit(model_input,y_train[:A.shape[-1]],epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "FUogle-Nw1Iv"
      },
      "id": "FUogle-Nw1Iv",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading in Graph Data and training SVD"
      ],
      "metadata": {
        "id": "iR290K6mvsJX"
      },
      "id": "iR290K6mvsJX"
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_test_edges(adj, test_frac=.1, val_frac=.05, prevent_disconnect=True, verbose=False):\n",
        "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "\n",
        "    if verbose == True:\n",
        "        print('preprocessing...')\n",
        "\n",
        "    # Remove diagonal elements\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "    adj.eliminate_zeros()\n",
        "    # Check that diag is zero:\n",
        "    assert np.diag(adj.todense()).sum() == 0\n",
        "\n",
        "    g = nx.from_scipy_sparse_matrix(adj)\n",
        "    orig_num_cc = nx.number_connected_components(g)\n",
        "\n",
        "    adj_triu = sp.triu(adj) # upper triangular portion of adj matrix\n",
        "    adj_tuple = sparse_to_tuple(adj_triu) # (coords, values, shape), edges only 1 way\n",
        "    edges = adj_tuple[0] # all edges, listed only once (not 2 ways)\n",
        "    # edges_all = sparse_to_tuple(adj)[0] # ALL edges (includes both ways)\n",
        "    num_test = int(np.floor(edges.shape[0] * test_frac)) # controls how large the test set should be\n",
        "    num_val = int(np.floor(edges.shape[0] * val_frac)) # controls how alrge the validation set should be\n",
        "\n",
        "    # Store edges in list of ordered tuples (node1, node2) where node1 < node2\n",
        "    edge_tuples = [(min(edge[0], edge[1]), max(edge[0], edge[1])) for edge in edges]\n",
        "    all_edge_tuples = set(edge_tuples)\n",
        "    train_edges = set(edge_tuples) # initialize train_edges to have all edges\n",
        "    test_edges = set()\n",
        "    val_edges = set()\n",
        "\n",
        "    if verbose == True:\n",
        "        print('generating test/val sets...')\n",
        "\n",
        "    # Iterate over shuffled edges, add to train/val sets\n",
        "    np.random.shuffle(edge_tuples)\n",
        "    for edge in edge_tuples:\n",
        "        # print(edge)\n",
        "        node1 = edge[0]\n",
        "        node2 = edge[1]\n",
        "\n",
        "        # If removing edge would disconnect a connected component, backtrack and move on\n",
        "        g.remove_edge(node1, node2)\n",
        "        if prevent_disconnect == True:\n",
        "            if nx.number_connected_components(g) > orig_num_cc:\n",
        "                g.add_edge(node1, node2)\n",
        "                continue\n",
        "\n",
        "        # Fill test_edges first\n",
        "        if len(test_edges) < num_test:\n",
        "            test_edges.add(edge)\n",
        "            train_edges.remove(edge)\n",
        "\n",
        "        # Then, fill val_edges\n",
        "        elif len(val_edges) < num_val:\n",
        "            val_edges.add(edge)\n",
        "            train_edges.remove(edge)\n",
        "\n",
        "        # Both edge lists full --> break loop\n",
        "        elif len(test_edges) == num_test and len(val_edges) == num_val:\n",
        "            break\n",
        "\n",
        "    if (len(val_edges) < num_val or len(test_edges) < num_test):\n",
        "        print(\"WARNING: not enough removable edges to perform full train-test split!\")\n",
        "        print(\"Num. (test, val) edges requested: (\", num_test, \", \", num_val, \")\")\n",
        "        print(\"Num. (test, val) edges returned: (\", len(test_edges), \", \", len(val_edges), \")\")\n",
        "\n",
        "    if prevent_disconnect == True:\n",
        "        assert nx.number_connected_components(g) == orig_num_cc\n",
        "\n",
        "    if verbose == True:\n",
        "        print('creating false test edges...')\n",
        "\n",
        "    test_edges_false = set()\n",
        "    while len(test_edges_false) < num_test:\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "\n",
        "        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n",
        "\n",
        "        # Make sure false_edge not an actual edge, and not a repeat\n",
        "        if false_edge in all_edge_tuples:\n",
        "            continue\n",
        "        if false_edge in test_edges_false:\n",
        "            continue\n",
        "\n",
        "        test_edges_false.add(false_edge)\n",
        "\n",
        "    if verbose == True:\n",
        "        print('creating false val edges...')\n",
        "\n",
        "    val_edges_false = set()\n",
        "    while len(val_edges_false) < num_val:\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "\n",
        "        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n",
        "\n",
        "        # Make sure false_edge in not an actual edge, not in test_edges_false, not a repeat\n",
        "        if false_edge in all_edge_tuples or \\\n",
        "            false_edge in test_edges_false or \\\n",
        "            false_edge in val_edges_false:\n",
        "            continue\n",
        "            \n",
        "        val_edges_false.add(false_edge)\n",
        "\n",
        "    if verbose == True:\n",
        "        print('creating false train edges...')\n",
        "\n",
        "    train_edges_false = set()\n",
        "    while len(train_edges_false) < len(train_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "\n",
        "        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n",
        "\n",
        "        # Make sure false_edge in not an actual edge, not in test_edges_false, \n",
        "            # not in val_edges_false, not a repeat\n",
        "        if false_edge in all_edge_tuples or \\\n",
        "            false_edge in test_edges_false or \\\n",
        "            false_edge in val_edges_false or \\\n",
        "            false_edge in train_edges_false:\n",
        "            continue\n",
        "\n",
        "        train_edges_false.add(false_edge)\n",
        "\n",
        "    if verbose == True:\n",
        "        print('final checks for disjointness...')\n",
        "\n",
        "    # assert: false_edges are actually false (not in all_edge_tuples)\n",
        "    assert test_edges_false.isdisjoint(all_edge_tuples)\n",
        "    assert val_edges_false.isdisjoint(all_edge_tuples)\n",
        "    assert train_edges_false.isdisjoint(all_edge_tuples)\n",
        "\n",
        "    # assert: test, val, train false edges disjoint\n",
        "    assert test_edges_false.isdisjoint(val_edges_false)\n",
        "    assert test_edges_false.isdisjoint(train_edges_false)\n",
        "    assert val_edges_false.isdisjoint(train_edges_false)\n",
        "\n",
        "    # assert: test, val, train positive edges disjoint\n",
        "    assert val_edges.isdisjoint(train_edges)\n",
        "    assert test_edges.isdisjoint(train_edges)\n",
        "    assert val_edges.isdisjoint(test_edges)\n",
        "\n",
        "    if verbose == True:\n",
        "        print('creating adj_train...')\n",
        "\n",
        "    # Re-build adj matrix using remaining graph\n",
        "    adj_train = nx.adjacency_matrix(g)\n",
        "\n",
        "    # Convert edge-lists to numpy arrays\n",
        "    train_edges = np.array([list(edge_tuple) for edge_tuple in train_edges])\n",
        "    train_edges_false = np.array([list(edge_tuple) for edge_tuple in train_edges_false])\n",
        "    val_edges = np.array([list(edge_tuple) for edge_tuple in val_edges])\n",
        "    val_edges_false = np.array([list(edge_tuple) for edge_tuple in val_edges_false])\n",
        "    test_edges = np.array([list(edge_tuple) for edge_tuple in test_edges])\n",
        "    test_edges_false = np.array([list(edge_tuple) for edge_tuple in test_edges_false])\n",
        "\n",
        "    if verbose == True:\n",
        "        print('Done with train-test split!')\n",
        "        print('')\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return adj_train, train_edges, train_edges_false, \\\n",
        "        val_edges, val_edges_false, test_edges, test_edges_false\n"
      ],
      "metadata": {
        "id": "2nr-HY1ACrWI"
      },
      "id": "2nr-HY1ACrWI",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse as sp"
      ],
      "metadata": {
        "id": "5sRHEx_HCy1j"
      },
      "id": "5sRHEx_HCy1j",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape\n",
        "    "
      ],
      "metadata": {
        "id": "f3nU6fmiC-sy"
      },
      "id": "f3nU6fmiC-sy",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0) # make sure train-test split is consistent between notebooks\n",
        "adj_sparse = nx.to_scipy_sparse_matrix(g)\n",
        "\n",
        "# Perform train-test split\n",
        "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
        "    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.1)\n",
        "g_train = nx.from_scipy_sparse_matrix(adj_train) # new graph object with only non-hidden edges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQHrt4gGBW83",
        "outputId": "17a3be1b-6277-4d38-be02-f9c1616871ed"
      },
      "id": "OQHrt4gGBW83",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: not enough removable edges to perform full train-test split!\n",
            "Num. (test, val) edges requested: ( 1994 ,  664 )\n",
            "Num. (test, val) edges returned: ( 1049 ,  0 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect train/test split\n",
        "print(\"Total nodes:\", adj_sparse.shape[0])\n",
        "print(\"Total edges:\", int(adj_sparse.nnz/2)) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\n",
        "print(\"Training edges (positive):\", len(train_edges))\n",
        "print(\"Training edges (negative):\", len(train_edges_false))\n",
        "print(\"Validation edges (positive):\", len(val_edges))\n",
        "print(\"Validation edges (negative):\", len(val_edges_false))\n",
        "print(\"Test edges (positive):\", len(test_edges))\n",
        "print(\"Test edges (negative):\", len(test_edges_false))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEQmZ_09DYzR",
        "outputId": "d0c22354-6fdd-443d-f6e5-b5dd8cab3ae6"
      },
      "id": "UEQmZ_09DYzR",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total nodes: 5599\n",
            "Total edges: 6647\n",
            "Training edges (positive): 5598\n",
            "Training edges (negative): 5598\n",
            "Validation edges (positive): 0\n",
            "Validation edges (negative): 664\n",
            "Test edges (positive): 1049\n",
            "Test edges (negative): 1994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_matr(g):\n",
        "    def f(pair):\n",
        "      new_dict = pair[1]\n",
        "      new_dict['ind'] = pair[0]\n",
        "      return new_dict\n",
        "\n",
        "    df = pd.DataFrame([f(x) for x in dict(g.nodes(data=True)).items()])\n",
        "    \n",
        "    needed_columns = list(df.columns)\n",
        "    needed_columns.remove('dumb_id')\n",
        "    needed_columns.remove('ind')\n",
        "\n",
        "    feature_matr = df[needed_columns].to_numpy()\n",
        "    print(f\"Raw feature_matr. shape: {feature_matr.shape}\\n{feature_matr}\")\n",
        "    \n",
        "\n",
        "    feature_matr = feature_matr - feature_matr.mean(axis=0)\n",
        "    feature_matr = feature_matr / feature_matr.std(axis=0)\n",
        "    feature_matr = feature_matr[:, np.any(~np.isnan(feature_matr), axis=0)]\n",
        "\n",
        "    assert feature_matr.shape[1] == int(feature_matr.std(axis=0).sum())\n",
        "\n",
        "    print(f\"Got {feature_matr.shape[1]} features after filtering\\n{feature_matr}\")\n",
        "    return feature_matr"
      ],
      "metadata": {
        "id": "A5rg9oPpDcyV"
      },
      "id": "A5rg9oPpDcyV",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matr = nx.adjacency_matrix(g_train)"
      ],
      "metadata": {
        "id": "-vJyvcNJDgGF"
      },
      "id": "-vJyvcNJDgGF",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svd_embedding(m, emb_size):\n",
        "    if sp.issparse(m):\n",
        "        m = m.todense()\n",
        "    U, S, VT = np.linalg.svd(m)\n",
        "    mean_diff = (U[:, :emb_size] - VT[:emb_size, :].T).mean()\n",
        " #   assert mean_diff < 1e-7, f\"Expected small difference in U and V. Got mean diff: {mean_diff}\"\n",
        "    return U[:, :emb_size]\n",
        "\n",
        "svd_emb = svd_embedding(feature_matr, emb_size=200)"
      ],
      "metadata": {
        "id": "BaezBLqQDi5I"
      },
      "id": "BaezBLqQDi5I",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "1287a29f",
      "metadata": {
        "id": "1287a29f"
      },
      "outputs": [],
      "source": [
        "#create embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "26ecb601",
      "metadata": {
        "id": "26ecb601"
      },
      "outputs": [],
      "source": [
        "# Create node embeddings matrix (rows = nodes, columns = embedding features)\n",
        "emb_list = []\n",
        "for node_index in range(0, adj_sparse.shape[0]):\n",
        "    node_str = str(node_index)\n",
        "    node_emb = np.squeeze(np.asarray(svd_emb[node_index]))\n",
        "    emb_list.append(node_emb)\n",
        "emb_matrix = np.vstack(emb_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "c6bce7de",
      "metadata": {
        "id": "c6bce7de"
      },
      "outputs": [],
      "source": [
        "# Generate bootstrapped edge embeddings (as is done in node2vec paper)\n",
        "# Edge embedding for (v1, v2) = hadamard product of node embeddings for v1, v2\n",
        "def get_edge_embeddings(edge_list):\n",
        "    embs = []\n",
        "    for edge in edge_list:\n",
        "        node1 = edge[0]\n",
        "        node2 = edge[1]\n",
        "        emb1 = emb_matrix[node1]\n",
        "        emb2 = emb_matrix[node2]\n",
        "        edge_emb = np.multiply(emb1, emb2)\n",
        "        embs.append(edge_emb)\n",
        "    embs = np.array(embs)\n",
        "    return embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "ec25448c",
      "metadata": {
        "id": "ec25448c"
      },
      "outputs": [],
      "source": [
        "# Train-set edge embeddings\n",
        "pos_train_edge_embs = get_edge_embeddings(train_edges)\n",
        "neg_train_edge_embs = get_edge_embeddings(train_edges_false)\n",
        "train_edge_embs = np.concatenate([pos_train_edge_embs, neg_train_edge_embs])\n",
        "\n",
        "# Create train-set edge labels: 1 = real edge, 0 = false edge\n",
        "train_edge_labels = np.concatenate([np.ones(len(train_edges)), np.zeros(len(train_edges_false))])\n",
        "\n",
        "# Val-set edge embeddings, labels\n",
        "pos_val_edge_embs = get_edge_embeddings(val_edges)\n",
        "neg_val_edge_embs = get_edge_embeddings(val_edges_false)\n",
        "#val_edge_embs = np.concatenate([pos_val_edge_embs, neg_val_edge_embs])\n",
        "val_edge_embs = neg_val_edge_embs\n",
        "val_edge_labels = np.concatenate([np.ones(len(val_edges)), np.zeros(len(val_edges_false))])\n",
        "\n",
        "# Test-set edge embeddings, labels\n",
        "pos_test_edge_embs = get_edge_embeddings(test_edges)\n",
        "neg_test_edge_embs = get_edge_embeddings(test_edges_false)\n",
        "test_edge_embs = np.concatenate([pos_test_edge_embs, neg_test_edge_embs])\n",
        "\n",
        "# Create val-set edge labels: 1 = real edge, 0 = false edge\n",
        "test_edge_labels = np.concatenate([np.ones(len(test_edges)), np.zeros(len(test_edges_false))])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression classifier on train-set edge embeddings\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "edge_classifier = LogisticRegression(random_state=0)\n",
        "edge_classifier.fit(train_edge_embs, train_edge_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTyN3usxD2KK",
        "outputId": "feb6ab05-5a67-4165-b417-f621cd256018"
      },
      "id": "rTyN3usxD2KK",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "val_preds = edge_classifier.predict_proba(val_edge_embs)[:, 1]\n",
        "#val_roc = roc_auc_score(val_edge_labels, val_preds)\n",
        "val_ap = average_precision_score(val_edge_labels, val_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aipM5jrlD5SH",
        "outputId": "2f1b9343-3f2d-4d79-ae9e-7970bab98a0f"
      },
      "id": "aipM5jrlD5SH",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py:864: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
        "test_preds = edge_classifier.predict_proba(test_edge_embs)[:, 1]\n",
        "test_roc = roc_auc_score(test_edge_labels, test_preds)\n",
        "test_ap = average_precision_score(test_edge_labels, test_preds)"
      ],
      "metadata": {
        "id": "TcnJ8rRVD5Ux"
      },
      "id": "TcnJ8rRVD5Ux",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('SVD Validation ROC score: ', str(val_roc))\n",
        "print('SVD Validation AP score: ', str(val_ap))\n",
        "print('SVD Test ROC score: ', str(test_roc))\n",
        "print('SVD Test AP score: ', str(test_ap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFZbjdOND5XD",
        "outputId": "16177f1a-f659-4ecb-db2d-6c60632122a0"
      },
      "id": "eFZbjdOND5XD",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVD Validation AP score:  nan\n",
            "SVD Test ROC score:  0.32742555598157674\n",
            "SVD Test AP score:  0.2869560446662278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "class edge_classification(nn.Module):\n",
        "  # pic -> embedding\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.prediction_stack = nn.Sequential(\n",
        "            nn.Linear(200, 200),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(p=0.2),\n",
        "            \n",
        "            nn.Linear(200, 200),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(p=0.2),\n",
        "\n",
        "            nn.Linear(200, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        predictions = self.prediction_stack(x)\n",
        "        return predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz4syezCD5Zo",
        "outputId": "ec4a3ba6-7380-4ae3-e7a1-adf99e81cd7f"
      },
      "id": "Pz4syezCD5Zo",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_edge_embs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGZihucVD5cK",
        "outputId": "5056e77b-e8e3-4769-afca-bb4e08932838"
      },
      "id": "AGZihucVD5cK",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def make_np_dataloader(edge_embs, labels, batch_size=100):\n",
        "    assert len(labels) == len(edge_embs)\n",
        "\n",
        "    data = np.hstack((edge_embs, labels[np.newaxis].T))\n",
        "\n",
        "    t = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    return DataLoader(t, batch_size=batch_size)\n",
        "\n",
        "train_dl = make_np_dataloader(train_edge_embs, train_edge_labels, batch_size=1000)\n",
        "test_dl = make_np_dataloader(test_edge_embs, test_edge_labels, batch_size=100_000)"
      ],
      "metadata": {
        "id": "QAKmsB-9D5e0"
      },
      "id": "QAKmsB-9D5e0",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(config):\n",
        "    config['model'].train()\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        losses = []\n",
        "        for batch, data in enumerate(train_dl):\n",
        "            X = data[:, :-1].to(device)\n",
        "            y = data[:, -1].to(torch.float32).to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = config['model'](X)\n",
        "            \n",
        "            pred_for_loss = pred.T[0]\n",
        "            gt_for_loss = y.to(torch.float32)\n",
        "\n",
        "            loss = config['loss_fn'](pred_for_loss, gt_for_loss)\n",
        "\n",
        "            # Backpropagation\n",
        "            \n",
        "            config['optimizer'].zero_grad()\n",
        "            loss.backward()\n",
        "            config['optimizer'].step()\n",
        "\n",
        "            loss = loss.item()\n",
        "            losses.append(loss)\n",
        "        print(f\"Epoch {epoch} avg loss: {np.mean(losses)}\")\n",
        "\n",
        "def test_loop(config):\n",
        "    config['model'].eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    losses = []\n",
        "    for data in test_dl:\n",
        "        X = data[:, :-1].to(device)\n",
        "        y = data[:, -1].to(torch.float32).to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = config['model'](X)\n",
        "        \n",
        "        pred_for_loss = pred.T[0]\n",
        "        gt_for_loss = y.to(torch.float32)\n",
        "\n",
        "        labels.append(gt_for_loss.cpu())\n",
        "        predictions.append(pred_for_loss.cpu())\n",
        "\n",
        "        loss = config['loss_fn'](pred_for_loss, gt_for_loss)\n",
        "\n",
        "        loss = loss.item()\n",
        "        losses.append(loss)\n",
        "    \n",
        "    all_labels = torch.cat(labels).detach().numpy()\n",
        "    all_predictions = torch.cat(predictions).detach().numpy()\n",
        "\n",
        "    eval_model(all_labels, all_predictions, test_edge_embs)\n",
        "    print(f\"    Avg loss: {np.mean(losses)}\")\n",
        "\n",
        "def eval_model(labels, predictions, edges):\n",
        "    val_roc = roc_auc_score(labels, predictions)\n",
        "    val_ap = average_precision_score(labels, predictions)\n",
        "\n",
        "    roc_score = roc_auc_score(labels, predictions)\n",
        "    ap_score = average_precision_score(labels, predictions)\n",
        "\n",
        "    print(f\"\"\"Test:\n",
        "    AP: {(ap_score):>0.3f} \n",
        "    ROC: {(roc_score):>0.3f}\"\"\")"
      ],
      "metadata": {
        "id": "v74cy9PuD5iR"
      },
      "id": "v74cy9PuD5iR",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'learning_rate': 5e-1,\n",
        "    'epochs': 50,\n",
        "    'big_epochs': 50,\n",
        "    'model': edge_classification().to(device),\n",
        "    'loss_fn': torch.nn.BCELoss(),\n",
        "}\n",
        "\n",
        "config['optimizer'] = torch.optim.SGD(config['model'].parameters(), lr=config['learning_rate'])"
      ],
      "metadata": {
        "id": "pJovPaQxEQ34"
      },
      "id": "pJovPaQxEQ34",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(config['big_epochs']):\n",
        "    print(f\"--------------BIG EPOCH {i}--------------\")\n",
        "    train_loop(config)\n",
        "    test_loop(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nyAe1L7EQ6i",
        "outputId": "dd269baa-4f38-4bba-c7ac-c5fc0f32161b"
      },
      "id": "2nyAe1L7EQ6i",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------BIG EPOCH 0--------------\n",
            "Epoch 0 avg loss: 0.6459476252396902\n",
            "Epoch 1 avg loss: 0.7212445785601934\n",
            "Epoch 2 avg loss: 0.7279457971453667\n",
            "Epoch 3 avg loss: 0.7284804607431093\n",
            "Epoch 4 avg loss: 0.7285203362504641\n",
            "Epoch 5 avg loss: 0.7285356024901072\n",
            "Epoch 6 avg loss: 0.7285363624493281\n",
            "Epoch 7 avg loss: 0.728527195751667\n",
            "Epoch 8 avg loss: 0.7285268778602282\n",
            "Epoch 9 avg loss: 0.7285234158237776\n",
            "Epoch 10 avg loss: 0.7285192956527075\n",
            "Epoch 11 avg loss: 0.7285138616959254\n",
            "Epoch 12 avg loss: 0.7285112366080284\n",
            "Epoch 13 avg loss: 0.7285130694508553\n",
            "Epoch 14 avg loss: 0.7285183916489283\n",
            "Epoch 15 avg loss: 0.7285101885596911\n",
            "Epoch 16 avg loss: 0.7285082414746284\n",
            "Epoch 17 avg loss: 0.7285138343771299\n",
            "Epoch 18 avg loss: 0.7285011013348898\n",
            "Epoch 19 avg loss: 0.7285107995073\n",
            "Epoch 20 avg loss: 0.7285046055912971\n",
            "Epoch 21 avg loss: 0.7285135264197985\n",
            "Epoch 22 avg loss: 0.7285129154721895\n",
            "Epoch 23 avg loss: 0.7285000731547674\n",
            "Epoch 24 avg loss: 0.728501483798027\n",
            "Epoch 25 avg loss: 0.7285063589612643\n",
            "Epoch 26 avg loss: 0.7285029565294584\n",
            "Epoch 27 avg loss: 0.7285034159819285\n",
            "Epoch 28 avg loss: 0.7285060187180837\n",
            "Epoch 29 avg loss: 0.7285086959600449\n",
            "Epoch 30 avg loss: 0.7285016253590584\n",
            "Epoch 31 avg loss: 0.7284969612956047\n",
            "Epoch 32 avg loss: 0.7285002594192823\n",
            "Epoch 33 avg loss: 0.728491780658563\n",
            "Epoch 34 avg loss: 0.7284963428974152\n",
            "Epoch 35 avg loss: 0.7284939413269361\n",
            "Epoch 36 avg loss: 0.7284923493862152\n",
            "Epoch 37 avg loss: 0.728498804072539\n",
            "Epoch 38 avg loss: 0.7284960051377615\n",
            "Epoch 39 avg loss: 0.728496233622233\n",
            "Epoch 40 avg loss: 0.7284991269310316\n",
            "Epoch 41 avg loss: 0.7284954736630121\n",
            "Epoch 42 avg loss: 0.728489488363266\n",
            "Epoch 43 avg loss: 0.7284952600797018\n",
            "Epoch 44 avg loss: 0.7284895603855451\n",
            "Epoch 45 avg loss: 0.7284888476133347\n",
            "Epoch 46 avg loss: 0.7284906804561615\n",
            "Epoch 47 avg loss: 0.7284968296686808\n",
            "Epoch 48 avg loss: 0.7284928311904272\n",
            "Epoch 49 avg loss: 0.728492317100366\n",
            "Test:\n",
            "    AP: 0.347 \n",
            "    ROC: 0.412\n",
            "    Avg loss: 0.6504985094070435\n",
            "--------------BIG EPOCH 1--------------\n",
            "Epoch 0 avg loss: 0.7284873351454735\n",
            "Epoch 1 avg loss: 0.7284927392999331\n",
            "Epoch 2 avg loss: 0.7284935812155405\n",
            "Epoch 3 avg loss: 0.7284859294692675\n",
            "Epoch 4 avg loss: 0.7284885694583257\n",
            "Epoch 5 avg loss: 0.7284834658106168\n",
            "Epoch 6 avg loss: 0.7284922624627749\n",
            "Epoch 7 avg loss: 0.7284901390473048\n",
            "Epoch 8 avg loss: 0.7284874568382899\n",
            "Epoch 9 avg loss: 0.728483721613884\n",
            "Epoch 10 avg loss: 0.7284866745273272\n",
            "Epoch 11 avg loss: 0.7284898310899734\n",
            "Epoch 12 avg loss: 0.7284801229834557\n",
            "Epoch 13 avg loss: 0.7284895678361257\n",
            "Epoch 14 avg loss: 0.7284852564334869\n",
            "Epoch 15 avg loss: 0.7284877374768257\n",
            "Epoch 16 avg loss: 0.7284827282031378\n",
            "Epoch 17 avg loss: 0.7284790600339571\n",
            "Epoch 18 avg loss: 0.7284828051924706\n",
            "Epoch 19 avg loss: 0.7284810667236646\n",
            "Epoch 20 avg loss: 0.7284796337286631\n",
            "Epoch 21 avg loss: 0.7284801254669825\n",
            "Epoch 22 avg loss: 0.7284756799538931\n",
            "Epoch 23 avg loss: 0.728478858868281\n",
            "Epoch 24 avg loss: 0.728477177520593\n",
            "Epoch 25 avg loss: 0.7284799764553705\n",
            "Epoch 26 avg loss: 0.7284763306379318\n",
            "Epoch 27 avg loss: 0.7284751161932945\n",
            "Epoch 28 avg loss: 0.728477954864502\n",
            "Epoch 29 avg loss: 0.7284805948535601\n",
            "Epoch 30 avg loss: 0.728476901849111\n",
            "Epoch 31 avg loss: 0.7284834459424019\n",
            "Epoch 32 avg loss: 0.7284793506066004\n",
            "Epoch 33 avg loss: 0.728475588063399\n",
            "Epoch 34 avg loss: 0.7284721384445826\n",
            "Epoch 35 avg loss: 0.728473057349523\n",
            "Epoch 36 avg loss: 0.7284762263298035\n",
            "Epoch 37 avg loss: 0.728476382791996\n",
            "Epoch 38 avg loss: 0.7284786428014437\n",
            "Epoch 39 avg loss: 0.7284683311978976\n",
            "Epoch 40 avg loss: 0.7284783745805422\n",
            "Epoch 41 avg loss: 0.7284756004810333\n",
            "Epoch 42 avg loss: 0.7284715746839842\n",
            "Epoch 43 avg loss: 0.7284755309422811\n",
            "Epoch 44 avg loss: 0.7284688130021095\n",
            "Epoch 45 avg loss: 0.7284713958700498\n",
            "Epoch 46 avg loss: 0.7284719944000244\n",
            "Epoch 47 avg loss: 0.7284750243028005\n",
            "Epoch 48 avg loss: 0.7284711500008901\n",
            "Epoch 49 avg loss: 0.7284718751907349\n",
            "Test:\n",
            "    AP: 0.348 \n",
            "    ROC: 0.420\n",
            "    Avg loss: 0.6504589319229126\n",
            "--------------BIG EPOCH 2--------------\n",
            "Epoch 0 avg loss: 0.7284711872537931\n",
            "Epoch 1 avg loss: 0.7284705763061842\n",
            "Epoch 2 avg loss: 0.728471539914608\n",
            "Epoch 3 avg loss: 0.7284685050447782\n",
            "Epoch 4 avg loss: 0.728468582034111\n",
            "Epoch 5 avg loss: 0.7284679984052976\n",
            "Epoch 6 avg loss: 0.7284665753444036\n",
            "Epoch 7 avg loss: 0.7284672011931738\n",
            "Epoch 8 avg loss: 0.7284696847200394\n",
            "Epoch 9 avg loss: 0.7284696102142334\n",
            "Epoch 10 avg loss: 0.7284677277008692\n",
            "Epoch 11 avg loss: 0.7284673104683558\n",
            "Epoch 12 avg loss: 0.7284683833519617\n",
            "Epoch 13 avg loss: 0.7284699430068334\n",
            "Epoch 14 avg loss: 0.7284654825925827\n",
            "Epoch 15 avg loss: 0.7284635653098425\n",
            "Epoch 16 avg loss: 0.7284654577573141\n",
            "Epoch 17 avg loss: 0.7284702584147453\n",
            "Epoch 18 avg loss: 0.7284689297278722\n",
            "Epoch 19 avg loss: 0.7284670645991961\n",
            "Epoch 20 avg loss: 0.728464774787426\n",
            "Epoch 21 avg loss: 0.7284683063626289\n",
            "Epoch 22 avg loss: 0.728467583656311\n",
            "Epoch 23 avg loss: 0.7284633591771126\n",
            "Epoch 24 avg loss: 0.728465644021829\n",
            "Epoch 25 avg loss: 0.7284630909562111\n",
            "Epoch 26 avg loss: 0.7284631555279096\n",
            "Epoch 27 avg loss: 0.7284619857867559\n",
            "Epoch 28 avg loss: 0.7284609004855156\n",
            "Epoch 29 avg loss: 0.7284667690594991\n",
            "Epoch 30 avg loss: 0.7284620925784111\n",
            "Epoch 31 avg loss: 0.7284614592790604\n",
            "Epoch 32 avg loss: 0.7284632499019305\n",
            "Epoch 33 avg loss: 0.7284625917673111\n",
            "Epoch 34 avg loss: 0.7284650430083275\n",
            "Epoch 35 avg loss: 0.7284659470121065\n",
            "Epoch 36 avg loss: 0.7284648641943932\n",
            "Epoch 37 avg loss: 0.7284643923242887\n",
            "Epoch 38 avg loss: 0.7284613301356634\n",
            "Epoch 39 avg loss: 0.7284593085447947\n",
            "Epoch 40 avg loss: 0.7284587149818739\n",
            "Epoch 41 avg loss: 0.7284616430600485\n",
            "Epoch 42 avg loss: 0.7284602547685305\n",
            "Epoch 43 avg loss: 0.7284650852282842\n",
            "Epoch 44 avg loss: 0.7284630164504051\n",
            "Epoch 45 avg loss: 0.728463406364123\n",
            "Epoch 46 avg loss: 0.7284622862935066\n",
            "Epoch 47 avg loss: 0.7284629642963409\n",
            "Epoch 48 avg loss: 0.7284612134099007\n",
            "Epoch 49 avg loss: 0.7284595519304276\n",
            "Test:\n",
            "    AP: 0.348 \n",
            "    ROC: 0.489\n",
            "    Avg loss: 0.6504343152046204\n",
            "--------------BIG EPOCH 3--------------\n",
            "Epoch 0 avg loss: 0.7284605825940768\n",
            "Epoch 1 avg loss: 0.7284618491927782\n",
            "Epoch 2 avg loss: 0.7284598847230276\n",
            "Epoch 3 avg loss: 0.7284596040844917\n",
            "Epoch 4 avg loss: 0.7284592216213545\n",
            "Epoch 5 avg loss: 0.7284587994217873\n",
            "Epoch 6 avg loss: 0.7284580916166306\n",
            "Epoch 7 avg loss: 0.7284611115852991\n",
            "Epoch 8 avg loss: 0.728458451728026\n",
            "Epoch 9 avg loss: 0.728457565108935\n",
            "Epoch 10 avg loss: 0.7284558415412903\n",
            "Epoch 11 avg loss: 0.7284591024120649\n",
            "Epoch 12 avg loss: 0.7284597580631574\n",
            "Epoch 13 avg loss: 0.7284562662243843\n",
            "Epoch 14 avg loss: 0.7284601728121439\n",
            "Epoch 15 avg loss: 0.7284568920731544\n",
            "Epoch 16 avg loss: 0.7284562190373739\n",
            "Epoch 17 avg loss: 0.728455883761247\n",
            "Epoch 18 avg loss: 0.7284568349520365\n",
            "Epoch 19 avg loss: 0.7284564326206843\n",
            "Epoch 20 avg loss: 0.7284555534521738\n",
            "Epoch 21 avg loss: 0.7284583896398544\n",
            "Epoch 22 avg loss: 0.7284582232435545\n",
            "Epoch 23 avg loss: 0.7284539764126142\n",
            "Epoch 24 avg loss: 0.7284563009937605\n",
            "Epoch 25 avg loss: 0.728455496331056\n",
            "Epoch 26 avg loss: 0.7284576147794724\n",
            "Epoch 27 avg loss: 0.7284561917185783\n",
            "Epoch 28 avg loss: 0.7284548580646515\n",
            "Epoch 29 avg loss: 0.7284551014502844\n",
            "Epoch 30 avg loss: 0.7284536436200142\n",
            "Epoch 31 avg loss: 0.7284541775782903\n",
            "Epoch 32 avg loss: 0.7284539391597112\n",
            "Epoch 33 avg loss: 0.7284556329250336\n",
            "Epoch 34 avg loss: 0.7284544656674067\n",
            "Epoch 35 avg loss: 0.728456956644853\n",
            "Epoch 36 avg loss: 0.7284546221295992\n",
            "Epoch 37 avg loss: 0.7284567207098007\n",
            "Epoch 38 avg loss: 0.728454053401947\n",
            "Epoch 39 avg loss: 0.728452650209268\n",
            "Epoch 40 avg loss: 0.7284539739290873\n",
            "Epoch 41 avg loss: 0.7284537603457769\n",
            "Epoch 42 avg loss: 0.7284532810250918\n",
            "Epoch 43 avg loss: 0.7284525235493978\n",
            "Epoch 44 avg loss: 0.728454572459062\n",
            "Epoch 45 avg loss: 0.7284534151355425\n",
            "Epoch 46 avg loss: 0.7284510905543963\n",
            "Epoch 47 avg loss: 0.7284553175171217\n",
            "Epoch 48 avg loss: 0.7284538596868515\n",
            "Epoch 49 avg loss: 0.7284530475735664\n",
            "Test:\n",
            "    AP: 0.346 \n",
            "    ROC: 0.406\n",
            "    Avg loss: 0.6504186987876892\n",
            "--------------BIG EPOCH 4--------------\n",
            "Epoch 0 avg loss: 0.7284519995252291\n",
            "Epoch 1 avg loss: 0.7284522900978724\n",
            "Epoch 2 avg loss: 0.7284540608525276\n",
            "Epoch 3 avg loss: 0.728451651831468\n",
            "Epoch 4 avg loss: 0.7284529705842336\n",
            "Epoch 5 avg loss: 0.7284518480300903\n",
            "Epoch 6 avg loss: 0.728452796737353\n",
            "Epoch 7 avg loss: 0.7284511004885038\n",
            "Epoch 8 avg loss: 0.7284504572550455\n",
            "Epoch 9 avg loss: 0.7284502411882082\n",
            "Epoch 10 avg loss: 0.7284523025155067\n",
            "Epoch 11 avg loss: 0.728452188273271\n",
            "Epoch 12 avg loss: 0.7284511774778366\n",
            "Epoch 13 avg loss: 0.7284493645032247\n",
            "Epoch 14 avg loss: 0.728453109661738\n",
            "Epoch 15 avg loss: 0.7284515226880709\n",
            "Epoch 16 avg loss: 0.7284514755010605\n",
            "Epoch 17 avg loss: 0.7284516294797262\n",
            "Epoch 18 avg loss: 0.728452223042647\n",
            "Epoch 19 avg loss: 0.7284502685070038\n",
            "Epoch 20 avg loss: 0.728451689084371\n",
            "Epoch 21 avg loss: 0.728451373676459\n",
            "Epoch 22 avg loss: 0.7284502411882082\n",
            "Epoch 23 avg loss: 0.7284506186842918\n",
            "Epoch 24 avg loss: 0.7284517760078112\n",
            "Epoch 25 avg loss: 0.7284508123993874\n",
            "Epoch 26 avg loss: 0.7284507205088934\n",
            "Epoch 27 avg loss: 0.7284501592318217\n",
            "Epoch 28 avg loss: 0.7284511650602022\n",
            "Epoch 29 avg loss: 0.7284492552280426\n",
            "Epoch 30 avg loss: 0.7284485076864561\n",
            "Epoch 31 avg loss: 0.7284500127037367\n",
            "Epoch 32 avg loss: 0.7284498736262321\n",
            "Epoch 33 avg loss: 0.728449617822965\n",
            "Epoch 34 avg loss: 0.728448823094368\n",
            "Epoch 35 avg loss: 0.7284493173162142\n",
            "Epoch 36 avg loss: 0.7284497569004694\n",
            "Epoch 37 avg loss: 0.7284511079390844\n",
            "Epoch 38 avg loss: 0.7284503703316053\n",
            "Epoch 39 avg loss: 0.7284474968910217\n",
            "Epoch 40 avg loss: 0.7284480283657709\n",
            "Epoch 41 avg loss: 0.7284487957755724\n",
            "Epoch 42 avg loss: 0.7284485250711441\n",
            "Epoch 43 avg loss: 0.7284491111834844\n",
            "Epoch 44 avg loss: 0.728447916607062\n",
            "Epoch 45 avg loss: 0.7284493297338486\n",
            "Epoch 46 avg loss: 0.7284494092067083\n",
            "Epoch 47 avg loss: 0.7284474993745486\n",
            "Epoch 48 avg loss: 0.7284488876660665\n",
            "Epoch 49 avg loss: 0.7284496227900187\n",
            "Test:\n",
            "    AP: 0.349 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6504088044166565\n",
            "--------------BIG EPOCH 5--------------\n",
            "Epoch 0 avg loss: 0.7284491310516993\n",
            "Epoch 1 avg loss: 0.7284495333830515\n",
            "Epoch 2 avg loss: 0.7284485250711441\n",
            "Epoch 3 avg loss: 0.7284479786952337\n",
            "Epoch 4 avg loss: 0.7284475093086561\n",
            "Epoch 5 avg loss: 0.7284477228919665\n",
            "Epoch 6 avg loss: 0.7284477750460306\n",
            "Epoch 7 avg loss: 0.7284480954209963\n",
            "Epoch 8 avg loss: 0.728446344534556\n",
            "Epoch 9 avg loss: 0.7284477899471918\n",
            "Epoch 10 avg loss: 0.7284480407834053\n",
            "Epoch 11 avg loss: 0.7284483835101128\n",
            "Epoch 12 avg loss: 0.7284476359685262\n",
            "Epoch 13 avg loss: 0.7284472659230232\n",
            "Epoch 14 avg loss: 0.7284480606516203\n",
            "Epoch 15 avg loss: 0.7284473329782486\n",
            "Epoch 16 avg loss: 0.7284476310014725\n",
            "Epoch 17 avg loss: 0.7284486815333366\n",
            "Epoch 18 avg loss: 0.7284485275546709\n",
            "Epoch 19 avg loss: 0.728447879354159\n",
            "Epoch 20 avg loss: 0.7284466251730919\n",
            "Epoch 21 avg loss: 0.7284480283657709\n",
            "Epoch 22 avg loss: 0.7284466450413069\n",
            "Epoch 23 avg loss: 0.7284477899471918\n",
            "Epoch 24 avg loss: 0.7284476906061172\n",
            "Epoch 25 avg loss: 0.7284469256798426\n",
            "Epoch 26 avg loss: 0.7284478197495142\n",
            "Epoch 27 avg loss: 0.7284469306468964\n",
            "Epoch 28 avg loss: 0.7284463147322336\n",
            "Epoch 29 avg loss: 0.7284470349550247\n",
            "Epoch 30 avg loss: 0.728446771701177\n",
            "Epoch 31 avg loss: 0.7284460365772247\n",
            "Epoch 32 avg loss: 0.7284466475248337\n",
            "Epoch 33 avg loss: 0.7284467617670695\n",
            "Epoch 34 avg loss: 0.728446883459886\n",
            "Epoch 35 avg loss: 0.7284468561410904\n",
            "Epoch 36 avg loss: 0.7284459446867307\n",
            "Epoch 37 avg loss: 0.7284465158979098\n",
            "Epoch 38 avg loss: 0.7284459695219994\n",
            "Epoch 39 avg loss: 0.7284454902013143\n",
            "Epoch 40 avg loss: 0.728445569674174\n",
            "Epoch 41 avg loss: 0.7284470846255621\n",
            "Epoch 42 avg loss: 0.7284461135665575\n",
            "Epoch 43 avg loss: 0.7284457087516785\n",
            "Epoch 44 avg loss: 0.7284453238050143\n",
            "Epoch 45 avg loss: 0.7284456913669904\n",
            "Epoch 46 avg loss: 0.7284455920259157\n",
            "Epoch 47 avg loss: 0.728445457915465\n",
            "Epoch 48 avg loss: 0.7284460638960203\n",
            "Epoch 49 avg loss: 0.7284455969929695\n",
            "Test:\n",
            "    AP: 0.347 \n",
            "    ROC: 0.497\n",
            "    Avg loss: 0.6504021286964417\n",
            "--------------BIG EPOCH 6--------------\n",
            "Epoch 0 avg loss: 0.7284454082449278\n",
            "Epoch 1 avg loss: 0.7284453411897024\n",
            "Epoch 2 avg loss: 0.7284453610579172\n",
            "Epoch 3 avg loss: 0.7284458056092262\n",
            "Epoch 4 avg loss: 0.7284457559386889\n",
            "Epoch 5 avg loss: 0.7284454827507337\n",
            "Epoch 6 avg loss: 0.7284453511238098\n",
            "Epoch 7 avg loss: 0.728445070485274\n",
            "Epoch 8 avg loss: 0.7284455547730128\n",
            "Epoch 9 avg loss: 0.7284455572565397\n",
            "Epoch 10 avg loss: 0.7284438063700994\n",
            "Epoch 11 avg loss: 0.7284445092082024\n",
            "Epoch 12 avg loss: 0.7284446482857069\n",
            "Epoch 13 avg loss: 0.7284441565473875\n",
            "Epoch 14 avg loss: 0.7284440249204636\n",
            "Epoch 15 avg loss: 0.7284446780880293\n",
            "Epoch 16 avg loss: 0.7284455349047979\n",
            "Epoch 17 avg loss: 0.7284437119960785\n",
            "Epoch 18 avg loss: 0.7284438212712606\n",
            "Epoch 19 avg loss: 0.7284443328777949\n",
            "Epoch 20 avg loss: 0.7284448593854904\n",
            "Epoch 21 avg loss: 0.728444424768289\n",
            "Epoch 22 avg loss: 0.7284446880221367\n",
            "Epoch 23 avg loss: 0.7284441590309143\n",
            "Epoch 24 avg loss: 0.728444737692674\n",
            "Epoch 25 avg loss: 0.7284432128071785\n",
            "Epoch 26 avg loss: 0.7284441714485487\n",
            "Epoch 27 avg loss: 0.7284441019097964\n",
            "Epoch 28 avg loss: 0.728443962832292\n",
            "Epoch 29 avg loss: 0.7284435505668322\n",
            "Epoch 30 avg loss: 0.7284444322188696\n",
            "Epoch 31 avg loss: 0.7284440944592158\n",
            "Epoch 32 avg loss: 0.728444459537665\n",
            "Epoch 33 avg loss: 0.7284440621733665\n",
            "Epoch 34 avg loss: 0.7284435058633486\n",
            "Epoch 35 avg loss: 0.7284441540638605\n",
            "Epoch 36 avg loss: 0.7284441490968069\n",
            "Epoch 37 avg loss: 0.7284431805213293\n",
            "Epoch 38 avg loss: 0.7284436821937561\n",
            "Epoch 39 avg loss: 0.728443923095862\n",
            "Epoch 40 avg loss: 0.7284437417984009\n",
            "Epoch 41 avg loss: 0.7284433344999949\n",
            "Epoch 42 avg loss: 0.7284438088536263\n",
            "Epoch 43 avg loss: 0.7284436176220576\n",
            "Epoch 44 avg loss: 0.7284434859951338\n",
            "Epoch 45 avg loss: 0.7284432798624039\n",
            "Epoch 46 avg loss: 0.7284431234002113\n",
            "Epoch 47 avg loss: 0.7284439032276472\n",
            "Epoch 48 avg loss: 0.7284433618187904\n",
            "Epoch 49 avg loss: 0.7284431780378023\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.457\n",
            "    Avg loss: 0.6503977179527283\n",
            "--------------BIG EPOCH 7--------------\n",
            "Epoch 0 avg loss: 0.7284427682558695\n",
            "Epoch 1 avg loss: 0.7284425968925158\n",
            "Epoch 2 avg loss: 0.7284433767199516\n",
            "Epoch 3 avg loss: 0.7284431879719099\n",
            "Epoch 4 avg loss: 0.7284427608052889\n",
            "Epoch 5 avg loss: 0.7284422765175501\n",
            "Epoch 6 avg loss: 0.7284432599941889\n",
            "Epoch 7 avg loss: 0.728442353506883\n",
            "Epoch 8 avg loss: 0.7284433394670486\n",
            "Epoch 9 avg loss: 0.7284427136182785\n",
            "Epoch 10 avg loss: 0.7284429023663203\n",
            "Epoch 11 avg loss: 0.7284424801667532\n",
            "Epoch 12 avg loss: 0.728442465265592\n",
            "Epoch 13 avg loss: 0.7284430290261904\n",
            "Epoch 14 avg loss: 0.7284421995282173\n",
            "Epoch 15 avg loss: 0.7284427185853323\n",
            "Epoch 16 avg loss: 0.7284429048498472\n",
            "Epoch 17 avg loss: 0.7284425273537636\n",
            "Epoch 18 avg loss: 0.7284425993760427\n",
            "Epoch 19 avg loss: 0.728442924718062\n",
            "Epoch 20 avg loss: 0.728443036476771\n",
            "Epoch 21 avg loss: 0.7284426564971606\n",
            "Epoch 22 avg loss: 0.7284422988692919\n",
            "Epoch 23 avg loss: 0.728442279001077\n",
            "Epoch 24 avg loss: 0.7284418766697248\n",
            "Epoch 25 avg loss: 0.7284421175718307\n",
            "Epoch 26 avg loss: 0.7284430960814158\n",
            "Epoch 27 avg loss: 0.7284425298372904\n",
            "Epoch 28 avg loss: 0.7284426564971606\n",
            "Epoch 29 avg loss: 0.7284423088033994\n",
            "Epoch 30 avg loss: 0.7284420107801756\n",
            "Epoch 31 avg loss: 0.7284427210688591\n",
            "Epoch 32 avg loss: 0.7284422243634859\n",
            "Epoch 33 avg loss: 0.7284420107801756\n",
            "Epoch 34 avg loss: 0.7284418319662412\n",
            "Epoch 35 avg loss: 0.7284414942065874\n",
            "Epoch 36 avg loss: 0.7284425695737203\n",
            "Epoch 37 avg loss: 0.7284419238567352\n",
            "Epoch 38 avg loss: 0.7284420306483904\n",
            "Epoch 39 avg loss: 0.7284416978557905\n",
            "Epoch 40 avg loss: 0.7284413427114487\n",
            "Epoch 41 avg loss: 0.7284423758586248\n",
            "Epoch 42 avg loss: 0.7284421622753143\n",
            "Epoch 43 avg loss: 0.7284414122502009\n",
            "Epoch 44 avg loss: 0.7284415314594904\n",
            "Epoch 45 avg loss: 0.7284424925843874\n",
            "Epoch 46 avg loss: 0.7284418269991875\n",
            "Epoch 47 avg loss: 0.728442020714283\n",
            "Epoch 48 avg loss: 0.7284424031774203\n",
            "Epoch 49 avg loss: 0.7284412359197935\n",
            "Test:\n",
            "    AP: 0.355 \n",
            "    ROC: 0.515\n",
            "    Avg loss: 0.6503943204879761\n",
            "--------------BIG EPOCH 8--------------\n",
            "Epoch 0 avg loss: 0.7284423212210337\n",
            "Epoch 1 avg loss: 0.7284415637453397\n",
            "Epoch 2 avg loss: 0.7284413327773412\n",
            "Epoch 3 avg loss: 0.7284418667356173\n",
            "Epoch 4 avg loss: 0.7284420281648636\n",
            "Epoch 5 avg loss: 0.7284410372376442\n",
            "Epoch 6 avg loss: 0.7284413675467173\n",
            "Epoch 7 avg loss: 0.7284417276581129\n",
            "Epoch 8 avg loss: 0.7284415637453397\n",
            "Epoch 9 avg loss: 0.7284419412414233\n",
            "Epoch 10 avg loss: 0.7284412880738577\n",
            "Epoch 11 avg loss: 0.728440893193086\n",
            "Epoch 12 avg loss: 0.7284418245156606\n",
            "Epoch 13 avg loss: 0.7284415240089098\n",
            "Epoch 14 avg loss: 0.7284416109323502\n",
            "Epoch 15 avg loss: 0.7284414197007815\n",
            "Epoch 16 avg loss: 0.7284415910641352\n",
            "Epoch 17 avg loss: 0.7284415538112322\n",
            "Epoch 18 avg loss: 0.7284414271513621\n",
            "Epoch 19 avg loss: 0.7284415091077486\n",
            "Epoch 20 avg loss: 0.7284413700302442\n",
            "Epoch 21 avg loss: 0.7284417276581129\n",
            "Epoch 22 avg loss: 0.7284412384033203\n",
            "Epoch 23 avg loss: 0.7284411564469337\n",
            "Epoch 24 avg loss: 0.7284412682056427\n",
            "Epoch 25 avg loss: 0.7284412135680517\n",
            "Epoch 26 avg loss: 0.7284414346019427\n",
            "Epoch 27 avg loss: 0.7284410446882248\n",
            "Epoch 28 avg loss: 0.7284410571058592\n",
            "Epoch 29 avg loss: 0.728441039721171\n",
            "Epoch 30 avg loss: 0.7284409006436666\n",
            "Epoch 31 avg loss: 0.7284409925341606\n",
            "Epoch 32 avg loss: 0.7284410744905472\n",
            "Epoch 33 avg loss: 0.7284409950176874\n",
            "Epoch 34 avg loss: 0.7284406051039696\n",
            "Epoch 35 avg loss: 0.7284408385554949\n",
            "Epoch 36 avg loss: 0.7284412284692129\n",
            "Epoch 37 avg loss: 0.7284407441814741\n",
            "Epoch 38 avg loss: 0.7284407640496889\n",
            "Epoch 39 avg loss: 0.7284405926863352\n",
            "Epoch 40 avg loss: 0.7284408311049143\n",
            "Epoch 41 avg loss: 0.7284405132134756\n",
            "Epoch 42 avg loss: 0.72844065229098\n",
            "Epoch 43 avg loss: 0.7284409205118815\n",
            "Epoch 44 avg loss: 0.7284408683578173\n",
            "Epoch 45 avg loss: 0.7284403219819069\n",
            "Epoch 46 avg loss: 0.7284408956766129\n",
            "Epoch 47 avg loss: 0.7284408609072367\n",
            "Epoch 48 avg loss: 0.728441022336483\n",
            "Epoch 49 avg loss: 0.7284407292803129\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.470\n",
            "    Avg loss: 0.650392472743988\n",
            "--------------BIG EPOCH 9--------------\n",
            "Epoch 0 avg loss: 0.7284409701824188\n",
            "Epoch 1 avg loss: 0.7284409826000532\n",
            "Epoch 2 avg loss: 0.7284410372376442\n",
            "Epoch 3 avg loss: 0.728440615038077\n",
            "Epoch 4 avg loss: 0.7284405256311098\n",
            "Epoch 5 avg loss: 0.7284410148859024\n",
            "Epoch 6 avg loss: 0.7284406696756681\n",
            "Epoch 7 avg loss: 0.7284409999847412\n",
            "Epoch 8 avg loss: 0.7284407243132591\n",
            "Epoch 9 avg loss: 0.7284408311049143\n",
            "Epoch 10 avg loss: 0.728440468509992\n",
            "Epoch 11 avg loss: 0.7284403964877129\n",
            "Epoch 12 avg loss: 0.7284407888849577\n",
            "Epoch 13 avg loss: 0.7284405504663786\n",
            "Epoch 14 avg loss: 0.7284403741359711\n",
            "Epoch 15 avg loss: 0.7284406945109367\n",
            "Epoch 16 avg loss: 0.7284403368830681\n",
            "Epoch 17 avg loss: 0.7284403095642725\n",
            "Epoch 18 avg loss: 0.7284403219819069\n",
            "Epoch 19 avg loss: 0.7284403294324875\n",
            "Epoch 20 avg loss: 0.7284403045972189\n",
            "Epoch 21 avg loss: 0.728440135717392\n",
            "Epoch 22 avg loss: 0.7284404585758845\n",
            "Epoch 23 avg loss: 0.7284402996301651\n",
            "Epoch 24 avg loss: 0.7284402747948965\n",
            "Epoch 25 avg loss: 0.7284404188394547\n",
            "Epoch 26 avg loss: 0.7284407069285711\n",
            "Epoch 27 avg loss: 0.7284406647086143\n",
            "Epoch 28 avg loss: 0.7284405330816904\n",
            "Epoch 29 avg loss: 0.7284404536088308\n",
            "Epoch 30 avg loss: 0.7284403617183367\n",
            "Epoch 31 avg loss: 0.7284399370352427\n",
            "Epoch 32 avg loss: 0.7284402052561442\n",
            "Epoch 33 avg loss: 0.7284399271011353\n",
            "Epoch 34 avg loss: 0.7284402797619501\n",
            "Epoch 35 avg loss: 0.7284399966398875\n",
            "Epoch 36 avg loss: 0.7284398997823397\n",
            "Epoch 37 avg loss: 0.7284401853879293\n",
            "Epoch 38 avg loss: 0.7284403592348099\n",
            "Epoch 39 avg loss: 0.7284400761127472\n",
            "Epoch 40 avg loss: 0.7284406746427218\n",
            "Epoch 41 avg loss: 0.7284398774305979\n",
            "Epoch 42 avg loss: 0.7284400835633278\n",
            "Epoch 43 avg loss: 0.7284402325749397\n",
            "Epoch 44 avg loss: 0.7284403045972189\n",
            "Epoch 45 avg loss: 0.7284402474761009\n",
            "Epoch 46 avg loss: 0.7284402425090472\n",
            "Epoch 47 avg loss: 0.7284400463104248\n",
            "Epoch 48 avg loss: 0.7284399097164472\n",
            "Epoch 49 avg loss: 0.7284399767716726\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503911018371582\n",
            "--------------BIG EPOCH 10--------------\n",
            "Epoch 0 avg loss: 0.7284399420022964\n",
            "Epoch 1 avg loss: 0.7284398898482323\n",
            "Epoch 2 avg loss: 0.7284400363763174\n",
            "Epoch 3 avg loss: 0.7284399196505547\n",
            "Epoch 4 avg loss: 0.7284402325749397\n",
            "Epoch 5 avg loss: 0.728439857562383\n",
            "Epoch 6 avg loss: 0.7284397756059965\n",
            "Epoch 7 avg loss: 0.7284399196505547\n",
            "Epoch 8 avg loss: 0.7284395595391592\n",
            "Epoch 9 avg loss: 0.7284399420022964\n",
            "Epoch 10 avg loss: 0.7284397408366203\n",
            "Epoch 11 avg loss: 0.7284399891893069\n",
            "Epoch 12 avg loss: 0.728439932068189\n",
            "Epoch 13 avg loss: 0.728439969321092\n",
            "Epoch 14 avg loss: 0.7284399668375651\n",
            "Epoch 15 avg loss: 0.7284401382009188\n",
            "Epoch 16 avg loss: 0.7284397209684054\n",
            "Epoch 17 avg loss: 0.7284398277600607\n",
            "Epoch 18 avg loss: 0.7284402325749397\n",
            "Epoch 19 avg loss: 0.728440115849177\n",
            "Epoch 20 avg loss: 0.7284398823976517\n",
            "Epoch 21 avg loss: 0.7284398923317591\n",
            "Epoch 22 avg loss: 0.7284397880236307\n",
            "Epoch 23 avg loss: 0.7284399469693502\n",
            "Epoch 24 avg loss: 0.7284397582213084\n",
            "Epoch 25 avg loss: 0.7284399146835009\n",
            "Epoch 26 avg loss: 0.7284398476282755\n",
            "Epoch 27 avg loss: 0.7284398103753725\n",
            "Epoch 28 avg loss: 0.728439673781395\n",
            "Epoch 29 avg loss: 0.7284397011001905\n",
            "Epoch 30 avg loss: 0.7284397780895233\n",
            "Epoch 31 avg loss: 0.7284398848811785\n",
            "Epoch 32 avg loss: 0.7284398153424263\n",
            "Epoch 33 avg loss: 0.7284397607048353\n",
            "Epoch 34 avg loss: 0.7284396464625994\n",
            "Epoch 35 avg loss: 0.7284396563967069\n",
            "Epoch 36 avg loss: 0.7284398029247919\n",
            "Epoch 37 avg loss: 0.7284397532542547\n",
            "Epoch 38 avg loss: 0.7284397731224695\n",
            "Epoch 39 avg loss: 0.7284397333860397\n",
            "Epoch 40 avg loss: 0.7284395918250084\n",
            "Epoch 41 avg loss: 0.7284394924839338\n",
            "Epoch 42 avg loss: 0.7284398426612219\n",
            "Epoch 43 avg loss: 0.7284397085507711\n",
            "Epoch 44 avg loss: 0.7284398650129636\n",
            "Epoch 45 avg loss: 0.7284399345517159\n",
            "Epoch 46 avg loss: 0.7284397383530935\n",
            "Epoch 47 avg loss: 0.7284396116932234\n",
            "Epoch 48 avg loss: 0.7284397631883621\n",
            "Epoch 49 avg loss: 0.7284396837155024\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503899097442627\n",
            "--------------BIG EPOCH 11--------------\n",
            "Epoch 0 avg loss: 0.7284395719567934\n",
            "Epoch 1 avg loss: 0.7284398749470711\n",
            "Epoch 2 avg loss: 0.728439581890901\n",
            "Epoch 3 avg loss: 0.7284396265943845\n",
            "Epoch 4 avg loss: 0.7284396886825562\n",
            "Epoch 5 avg loss: 0.7284397110342979\n",
            "Epoch 6 avg loss: 0.7284396688143412\n",
            "Epoch 7 avg loss: 0.7284397482872009\n",
            "Epoch 8 avg loss: 0.728439616660277\n",
            "Epoch 9 avg loss: 0.7284397358695666\n",
            "Epoch 10 avg loss: 0.7284398848811785\n",
            "Epoch 11 avg loss: 0.7284397135178248\n",
            "Epoch 12 avg loss: 0.7284397607048353\n",
            "Epoch 13 avg loss: 0.7284396464625994\n",
            "Epoch 14 avg loss: 0.7284398153424263\n",
            "Epoch 15 avg loss: 0.7284397905071577\n",
            "Epoch 16 avg loss: 0.7284395520885786\n",
            "Epoch 17 avg loss: 0.7284395222862562\n",
            "Epoch 18 avg loss: 0.7284396141767502\n",
            "Epoch 19 avg loss: 0.7284396986166636\n",
            "Epoch 20 avg loss: 0.7284397160013517\n",
            "Epoch 21 avg loss: 0.7284397780895233\n",
            "Epoch 22 avg loss: 0.7284396216273308\n",
            "Epoch 23 avg loss: 0.7284398501118025\n",
            "Epoch 24 avg loss: 0.7284397035837173\n",
            "Epoch 25 avg loss: 0.7284395893414816\n",
            "Epoch 26 avg loss: 0.7284397011001905\n",
            "Epoch 27 avg loss: 0.7284397681554159\n",
            "Epoch 28 avg loss: 0.7284395694732666\n",
            "Epoch 29 avg loss: 0.7284395098686218\n",
            "Epoch 30 avg loss: 0.728439507385095\n",
            "Epoch 31 avg loss: 0.7284398054083189\n",
            "Epoch 32 avg loss: 0.7284396688143412\n",
            "Epoch 33 avg loss: 0.7284395893414816\n",
            "Epoch 34 avg loss: 0.7284394204616547\n",
            "Epoch 35 avg loss: 0.728439728418986\n",
            "Epoch 36 avg loss: 0.7284396936496099\n",
            "Epoch 37 avg loss: 0.7284395645062128\n",
            "Epoch 38 avg loss: 0.7284396290779114\n",
            "Epoch 39 avg loss: 0.7284394452969233\n",
            "Epoch 40 avg loss: 0.7284395570556322\n",
            "Epoch 41 avg loss: 0.7284395719567934\n",
            "Epoch 42 avg loss: 0.7284395049015681\n",
            "Epoch 43 avg loss: 0.7284396390120188\n",
            "Epoch 44 avg loss: 0.7284394726157188\n",
            "Epoch 45 avg loss: 0.728439673781395\n",
            "Epoch 46 avg loss: 0.7284396141767502\n",
            "Epoch 47 avg loss: 0.7284395967920622\n",
            "Epoch 48 avg loss: 0.728439636528492\n",
            "Epoch 49 avg loss: 0.728439599275589\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503896713256836\n",
            "--------------BIG EPOCH 12--------------\n",
            "Epoch 0 avg loss: 0.7284395893414816\n",
            "Epoch 1 avg loss: 0.7284396092096964\n",
            "Epoch 2 avg loss: 0.7284394825498263\n",
            "Epoch 3 avg loss: 0.7284394552310308\n",
            "Epoch 4 avg loss: 0.728439579407374\n",
            "Epoch 5 avg loss: 0.7284396489461263\n",
            "Epoch 6 avg loss: 0.728439507385095\n",
            "Epoch 7 avg loss: 0.7284396464625994\n",
            "Epoch 8 avg loss: 0.728439542154471\n",
            "Epoch 9 avg loss: 0.7284395322203636\n",
            "Epoch 10 avg loss: 0.7284396390120188\n",
            "Epoch 11 avg loss: 0.7284394900004069\n",
            "Epoch 12 avg loss: 0.728439616660277\n",
            "Epoch 13 avg loss: 0.7284396116932234\n",
            "Epoch 14 avg loss: 0.7284395719567934\n",
            "Epoch 15 avg loss: 0.7284396439790726\n",
            "Epoch 16 avg loss: 0.728439544637998\n",
            "Epoch 17 avg loss: 0.7284395098686218\n",
            "Epoch 18 avg loss: 0.7284395719567934\n",
            "Epoch 19 avg loss: 0.7284394527475039\n",
            "Epoch 20 avg loss: 0.7284395967920622\n",
            "Epoch 21 avg loss: 0.7284397085507711\n",
            "Epoch 22 avg loss: 0.7284395943085352\n",
            "Epoch 23 avg loss: 0.7284395719567934\n",
            "Epoch 24 avg loss: 0.7284394750992457\n",
            "Epoch 25 avg loss: 0.728439544637998\n",
            "Epoch 26 avg loss: 0.7284395744403204\n",
            "Epoch 27 avg loss: 0.7284396017591158\n",
            "Epoch 28 avg loss: 0.7284394601980845\n",
            "Epoch 29 avg loss: 0.7284396067261696\n",
            "Epoch 30 avg loss: 0.7284395173192024\n",
            "Epoch 31 avg loss: 0.7284396663308144\n",
            "Epoch 32 avg loss: 0.728439524769783\n",
            "Epoch 33 avg loss: 0.7284395024180412\n",
            "Epoch 34 avg loss: 0.7284397085507711\n",
            "Epoch 35 avg loss: 0.7284395297368368\n",
            "Epoch 36 avg loss: 0.7284395843744278\n",
            "Epoch 37 avg loss: 0.7284396464625994\n",
            "Epoch 38 avg loss: 0.7284395694732666\n",
            "Epoch 39 avg loss: 0.7284395669897398\n",
            "Epoch 40 avg loss: 0.728439524769783\n",
            "Epoch 41 avg loss: 0.7284395098686218\n",
            "Epoch 42 avg loss: 0.728439470132192\n",
            "Epoch 43 avg loss: 0.7284395719567934\n",
            "Epoch 44 avg loss: 0.7284395943085352\n",
            "Epoch 45 avg loss: 0.7284394825498263\n",
            "Epoch 46 avg loss: 0.7284395719567934\n",
            "Epoch 47 avg loss: 0.7284394527475039\n",
            "Epoch 48 avg loss: 0.7284395570556322\n",
            "Epoch 49 avg loss: 0.7284395396709442\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503898501396179\n",
            "--------------BIG EPOCH 13--------------\n",
            "Epoch 0 avg loss: 0.7284396241108576\n",
            "Epoch 1 avg loss: 0.7284395868579546\n",
            "Epoch 2 avg loss: 0.7284396017591158\n",
            "Epoch 3 avg loss: 0.728439450263977\n",
            "Epoch 4 avg loss: 0.7284395123521487\n",
            "Epoch 5 avg loss: 0.7284396216273308\n",
            "Epoch 6 avg loss: 0.7284394452969233\n",
            "Epoch 7 avg loss: 0.7284395520885786\n",
            "Epoch 8 avg loss: 0.7284395520885786\n",
            "Epoch 9 avg loss: 0.7284396315614382\n",
            "Epoch 10 avg loss: 0.7284395918250084\n",
            "Epoch 11 avg loss: 0.7284395123521487\n",
            "Epoch 12 avg loss: 0.7284395049015681\n",
            "Epoch 13 avg loss: 0.7284395222862562\n",
            "Epoch 14 avg loss: 0.7284394999345144\n",
            "Epoch 15 avg loss: 0.7284396216273308\n",
            "Epoch 16 avg loss: 0.7284395893414816\n",
            "Epoch 17 avg loss: 0.7284396042426428\n",
            "Epoch 18 avg loss: 0.7284394477804502\n",
            "Epoch 19 avg loss: 0.7284394924839338\n",
            "Epoch 20 avg loss: 0.7284394850333532\n",
            "Epoch 21 avg loss: 0.7284395396709442\n",
            "Epoch 22 avg loss: 0.7284396092096964\n",
            "Epoch 23 avg loss: 0.7284396464625994\n",
            "Epoch 24 avg loss: 0.7284396141767502\n",
            "Epoch 25 avg loss: 0.7284395148356756\n",
            "Epoch 26 avg loss: 0.7284395868579546\n",
            "Epoch 27 avg loss: 0.7284394428133965\n",
            "Epoch 28 avg loss: 0.7284397110342979\n",
            "Epoch 29 avg loss: 0.7284396216273308\n",
            "Epoch 30 avg loss: 0.7284395148356756\n",
            "Epoch 31 avg loss: 0.7284394949674606\n",
            "Epoch 32 avg loss: 0.7284394477804502\n",
            "Epoch 33 avg loss: 0.7284396514296532\n",
            "Epoch 34 avg loss: 0.7284395719567934\n",
            "Epoch 35 avg loss: 0.7284395719567934\n",
            "Epoch 36 avg loss: 0.7284395496050516\n",
            "Epoch 37 avg loss: 0.7284395744403204\n",
            "Epoch 38 avg loss: 0.7284395843744278\n",
            "Epoch 39 avg loss: 0.7284395893414816\n",
            "Epoch 40 avg loss: 0.7284395545721054\n",
            "Epoch 41 avg loss: 0.7284396216273308\n",
            "Epoch 42 avg loss: 0.7284394974509875\n",
            "Epoch 43 avg loss: 0.728439544637998\n",
            "Epoch 44 avg loss: 0.7284394775827726\n",
            "Epoch 45 avg loss: 0.7284394974509875\n",
            "Epoch 46 avg loss: 0.7284394378463427\n",
            "Epoch 47 avg loss: 0.7284394353628159\n",
            "Epoch 48 avg loss: 0.728439470132192\n",
            "Epoch 49 avg loss: 0.72843948751688\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.501\n",
            "    Avg loss: 0.6503899097442627\n",
            "--------------BIG EPOCH 14--------------\n",
            "Epoch 0 avg loss: 0.7284394974509875\n",
            "Epoch 1 avg loss: 0.7284394775827726\n",
            "Epoch 2 avg loss: 0.7284394601980845\n",
            "Epoch 3 avg loss: 0.72843948751688\n",
            "Epoch 4 avg loss: 0.7284394403298696\n",
            "Epoch 5 avg loss: 0.7284396092096964\n",
            "Epoch 6 avg loss: 0.7284396290779114\n",
            "Epoch 7 avg loss: 0.7284395049015681\n",
            "Epoch 8 avg loss: 0.7284396116932234\n",
            "Epoch 9 avg loss: 0.728439544637998\n",
            "Epoch 10 avg loss: 0.7284395645062128\n",
            "Epoch 11 avg loss: 0.7284393534064293\n",
            "Epoch 12 avg loss: 0.7284395918250084\n",
            "Epoch 13 avg loss: 0.7284395148356756\n",
            "Epoch 14 avg loss: 0.7284395098686218\n",
            "Epoch 15 avg loss: 0.728439542154471\n",
            "Epoch 16 avg loss: 0.7284395868579546\n",
            "Epoch 17 avg loss: 0.7284395123521487\n",
            "Epoch 18 avg loss: 0.7284394626816114\n",
            "Epoch 19 avg loss: 0.7284395719567934\n",
            "Epoch 20 avg loss: 0.7284396514296532\n",
            "Epoch 21 avg loss: 0.7284395570556322\n",
            "Epoch 22 avg loss: 0.7284394303957621\n",
            "Epoch 23 avg loss: 0.7284394676486651\n",
            "Epoch 24 avg loss: 0.7284394179781278\n",
            "Epoch 25 avg loss: 0.728439542154471\n",
            "Epoch 26 avg loss: 0.7284396315614382\n",
            "Epoch 27 avg loss: 0.7284395272533098\n",
            "Epoch 28 avg loss: 0.7284396216273308\n",
            "Epoch 29 avg loss: 0.7284395694732666\n",
            "Epoch 30 avg loss: 0.7284394378463427\n",
            "Epoch 31 avg loss: 0.7284394900004069\n",
            "Epoch 32 avg loss: 0.728439507385095\n",
            "Epoch 33 avg loss: 0.7284395520885786\n",
            "Epoch 34 avg loss: 0.728439579407374\n",
            "Epoch 35 avg loss: 0.7284394452969233\n",
            "Epoch 36 avg loss: 0.7284394080440203\n",
            "Epoch 37 avg loss: 0.7284396936496099\n",
            "Epoch 38 avg loss: 0.7284396663308144\n",
            "Epoch 39 avg loss: 0.7284397110342979\n",
            "Epoch 40 avg loss: 0.7284395396709442\n",
            "Epoch 41 avg loss: 0.7284394179781278\n",
            "Epoch 42 avg loss: 0.7284394850333532\n",
            "Epoch 43 avg loss: 0.7284394204616547\n",
            "Epoch 44 avg loss: 0.7284394750992457\n",
            "Epoch 45 avg loss: 0.7284394900004069\n",
            "Epoch 46 avg loss: 0.7284395148356756\n",
            "Epoch 47 avg loss: 0.7284396067261696\n",
            "Epoch 48 avg loss: 0.7284395098686218\n",
            "Epoch 49 avg loss: 0.7284394378463427\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503899097442627\n",
            "--------------BIG EPOCH 15--------------\n",
            "Epoch 0 avg loss: 0.7284395918250084\n",
            "Epoch 1 avg loss: 0.7284394452969233\n",
            "Epoch 2 avg loss: 0.7284393807252248\n",
            "Epoch 3 avg loss: 0.7284395322203636\n",
            "Epoch 4 avg loss: 0.7284396216273308\n",
            "Epoch 5 avg loss: 0.7284394900004069\n",
            "Epoch 6 avg loss: 0.7284395520885786\n",
            "Epoch 7 avg loss: 0.7284394378463427\n",
            "Epoch 8 avg loss: 0.7284395843744278\n",
            "Epoch 9 avg loss: 0.728439562022686\n",
            "Epoch 10 avg loss: 0.7284396489461263\n",
            "Epoch 11 avg loss: 0.7284395123521487\n",
            "Epoch 12 avg loss: 0.7284395396709442\n",
            "Epoch 13 avg loss: 0.7284396067261696\n",
            "Epoch 14 avg loss: 0.7284396340449651\n",
            "Epoch 15 avg loss: 0.728439544637998\n",
            "Epoch 16 avg loss: 0.7284395396709442\n",
            "Epoch 17 avg loss: 0.7284395893414816\n",
            "Epoch 18 avg loss: 0.7284394105275472\n",
            "Epoch 19 avg loss: 0.7284395868579546\n",
            "Epoch 20 avg loss: 0.728439599275589\n",
            "Epoch 21 avg loss: 0.728439599275589\n",
            "Epoch 22 avg loss: 0.728439450263977\n",
            "Epoch 23 avg loss: 0.7284394626816114\n",
            "Epoch 24 avg loss: 0.7284395297368368\n",
            "Epoch 25 avg loss: 0.7284394924839338\n",
            "Epoch 26 avg loss: 0.7284394651651382\n",
            "Epoch 27 avg loss: 0.7284395520885786\n",
            "Epoch 28 avg loss: 0.7284394949674606\n",
            "Epoch 29 avg loss: 0.7284395645062128\n",
            "Epoch 30 avg loss: 0.7284396116932234\n",
            "Epoch 31 avg loss: 0.7284393732746443\n",
            "Epoch 32 avg loss: 0.7284396563967069\n",
            "Epoch 33 avg loss: 0.7284394130110741\n",
            "Epoch 34 avg loss: 0.72843948751688\n",
            "Epoch 35 avg loss: 0.7284396762649218\n",
            "Epoch 36 avg loss: 0.72843965391318\n",
            "Epoch 37 avg loss: 0.7284396042426428\n",
            "Epoch 38 avg loss: 0.7284396092096964\n",
            "Epoch 39 avg loss: 0.7284396092096964\n",
            "Epoch 40 avg loss: 0.7284394477804502\n",
            "Epoch 41 avg loss: 0.7284396290779114\n",
            "Epoch 42 avg loss: 0.7284396588802338\n",
            "Epoch 43 avg loss: 0.728439579407374\n",
            "Epoch 44 avg loss: 0.728439507385095\n",
            "Epoch 45 avg loss: 0.7284394949674606\n",
            "Epoch 46 avg loss: 0.7284395694732666\n",
            "Epoch 47 avg loss: 0.7284396141767502\n",
            "Epoch 48 avg loss: 0.7284395645062128\n",
            "Epoch 49 avg loss: 0.7284395545721054\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503896713256836\n",
            "--------------BIG EPOCH 16--------------\n",
            "Epoch 0 avg loss: 0.7284396663308144\n",
            "Epoch 1 avg loss: 0.7284395893414816\n",
            "Epoch 2 avg loss: 0.7284395123521487\n",
            "Epoch 3 avg loss: 0.7284395967920622\n",
            "Epoch 4 avg loss: 0.7284395123521487\n",
            "Epoch 5 avg loss: 0.7284395496050516\n",
            "Epoch 6 avg loss: 0.7284395471215248\n",
            "Epoch 7 avg loss: 0.728439450263977\n",
            "Epoch 8 avg loss: 0.7284396042426428\n",
            "Epoch 9 avg loss: 0.7284394850333532\n",
            "Epoch 10 avg loss: 0.728439395626386\n",
            "Epoch 11 avg loss: 0.7284395371874174\n",
            "Epoch 12 avg loss: 0.7284394949674606\n",
            "Epoch 13 avg loss: 0.7284396042426428\n",
            "Epoch 14 avg loss: 0.728439470132192\n",
            "Epoch 15 avg loss: 0.7284393981099129\n",
            "Epoch 16 avg loss: 0.7284395868579546\n",
            "Epoch 17 avg loss: 0.7284395322203636\n",
            "Epoch 18 avg loss: 0.7284396116932234\n",
            "Epoch 19 avg loss: 0.7284395123521487\n",
            "Epoch 20 avg loss: 0.7284395272533098\n",
            "Epoch 21 avg loss: 0.7284395198027293\n",
            "Epoch 22 avg loss: 0.7284394924839338\n",
            "Epoch 23 avg loss: 0.7284397706389427\n",
            "Epoch 24 avg loss: 0.7284394974509875\n",
            "Epoch 25 avg loss: 0.7284396265943845\n",
            "Epoch 26 avg loss: 0.7284395645062128\n",
            "Epoch 27 avg loss: 0.728439581890901\n",
            "Epoch 28 avg loss: 0.7284393931428591\n",
            "Epoch 29 avg loss: 0.7284394924839338\n",
            "Epoch 30 avg loss: 0.7284394850333532\n",
            "Epoch 31 avg loss: 0.728439619143804\n",
            "Epoch 32 avg loss: 0.728439470132192\n",
            "Epoch 33 avg loss: 0.7284395868579546\n",
            "Epoch 34 avg loss: 0.7284394452969233\n",
            "Epoch 35 avg loss: 0.72843965391318\n",
            "Epoch 36 avg loss: 0.7284397060672442\n",
            "Epoch 37 avg loss: 0.728439432879289\n",
            "Epoch 38 avg loss: 0.7284395272533098\n",
            "Epoch 39 avg loss: 0.7284395520885786\n",
            "Epoch 40 avg loss: 0.728439470132192\n",
            "Epoch 41 avg loss: 0.728439507385095\n",
            "Epoch 42 avg loss: 0.7284395918250084\n",
            "Epoch 43 avg loss: 0.7284395173192024\n",
            "Epoch 44 avg loss: 0.7284393931428591\n",
            "Epoch 45 avg loss: 0.7284394527475039\n",
            "Epoch 46 avg loss: 0.7284396141767502\n",
            "Epoch 47 avg loss: 0.7284396216273308\n",
            "Epoch 48 avg loss: 0.7284395694732666\n",
            "Epoch 49 avg loss: 0.7284396464625994\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503900289535522\n",
            "--------------BIG EPOCH 17--------------\n",
            "Epoch 0 avg loss: 0.7284397011001905\n",
            "Epoch 1 avg loss: 0.7284396315614382\n",
            "Epoch 2 avg loss: 0.7284394974509875\n",
            "Epoch 3 avg loss: 0.7284395148356756\n",
            "Epoch 4 avg loss: 0.7284395272533098\n",
            "Epoch 5 avg loss: 0.7284394179781278\n",
            "Epoch 6 avg loss: 0.7284395297368368\n",
            "Epoch 7 avg loss: 0.7284395645062128\n",
            "Epoch 8 avg loss: 0.7284395024180412\n",
            "Epoch 9 avg loss: 0.7284395893414816\n",
            "Epoch 10 avg loss: 0.7284394353628159\n",
            "Epoch 11 avg loss: 0.7284394800662994\n",
            "Epoch 12 avg loss: 0.7284395098686218\n",
            "Epoch 13 avg loss: 0.7284394750992457\n",
            "Epoch 14 avg loss: 0.7284394577145576\n",
            "Epoch 15 avg loss: 0.7284395669897398\n",
            "Epoch 16 avg loss: 0.7284394601980845\n",
            "Epoch 17 avg loss: 0.7284395024180412\n",
            "Epoch 18 avg loss: 0.7284395148356756\n",
            "Epoch 19 avg loss: 0.728439581890901\n",
            "Epoch 20 avg loss: 0.7284396265943845\n",
            "Epoch 21 avg loss: 0.7284395297368368\n",
            "Epoch 22 avg loss: 0.7284394353628159\n",
            "Epoch 23 avg loss: 0.7284396017591158\n",
            "Epoch 24 avg loss: 0.7284396241108576\n",
            "Epoch 25 avg loss: 0.7284396092096964\n",
            "Epoch 26 avg loss: 0.7284394105275472\n",
            "Epoch 27 avg loss: 0.7284395347038904\n",
            "Epoch 28 avg loss: 0.7284395744403204\n",
            "Epoch 29 avg loss: 0.7284392441312472\n",
            "Epoch 30 avg loss: 0.7284396613637606\n",
            "Epoch 31 avg loss: 0.7284395645062128\n",
            "Epoch 32 avg loss: 0.7284396489461263\n",
            "Epoch 33 avg loss: 0.7284394900004069\n",
            "Epoch 34 avg loss: 0.7284395173192024\n",
            "Epoch 35 avg loss: 0.7284394030769666\n",
            "Epoch 36 avg loss: 0.7284394080440203\n",
            "Epoch 37 avg loss: 0.7284395769238472\n",
            "Epoch 38 avg loss: 0.7284395322203636\n",
            "Epoch 39 avg loss: 0.7284396563967069\n",
            "Epoch 40 avg loss: 0.7284394030769666\n",
            "Epoch 41 avg loss: 0.7284394030769666\n",
            "Epoch 42 avg loss: 0.7284396414955457\n",
            "Epoch 43 avg loss: 0.7284395967920622\n",
            "Epoch 44 avg loss: 0.7284394999345144\n",
            "Epoch 45 avg loss: 0.7284395297368368\n",
            "Epoch 46 avg loss: 0.7284395496050516\n",
            "Epoch 47 avg loss: 0.7284395396709442\n",
            "Epoch 48 avg loss: 0.7284394900004069\n",
            "Epoch 49 avg loss: 0.7284393782416979\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503896713256836\n",
            "--------------BIG EPOCH 18--------------\n",
            "Epoch 0 avg loss: 0.7284396638472875\n",
            "Epoch 1 avg loss: 0.7284394775827726\n",
            "Epoch 2 avg loss: 0.7284395198027293\n",
            "Epoch 3 avg loss: 0.7284394080440203\n",
            "Epoch 4 avg loss: 0.7284394750992457\n",
            "Epoch 5 avg loss: 0.7284395943085352\n",
            "Epoch 6 avg loss: 0.728439581890901\n",
            "Epoch 7 avg loss: 0.7284394254287084\n",
            "Epoch 8 avg loss: 0.7284395943085352\n",
            "Epoch 9 avg loss: 0.7284396265943845\n",
            "Epoch 10 avg loss: 0.7284394030769666\n",
            "Epoch 11 avg loss: 0.7284395371874174\n",
            "Epoch 12 avg loss: 0.7284395744403204\n",
            "Epoch 13 avg loss: 0.7284395520885786\n",
            "Epoch 14 avg loss: 0.7284395272533098\n",
            "Epoch 15 avg loss: 0.7284395496050516\n",
            "Epoch 16 avg loss: 0.7284396092096964\n",
            "Epoch 17 avg loss: 0.7284395148356756\n",
            "Epoch 18 avg loss: 0.7284396588802338\n",
            "Epoch 19 avg loss: 0.7284395669897398\n",
            "Epoch 20 avg loss: 0.7284393732746443\n",
            "Epoch 21 avg loss: 0.7284394750992457\n",
            "Epoch 22 avg loss: 0.7284395868579546\n",
            "Epoch 23 avg loss: 0.7284394130110741\n",
            "Epoch 24 avg loss: 0.7284395471215248\n",
            "Epoch 25 avg loss: 0.7284396241108576\n",
            "Epoch 26 avg loss: 0.7284395545721054\n",
            "Epoch 27 avg loss: 0.7284393409887949\n",
            "Epoch 28 avg loss: 0.728439470132192\n",
            "Epoch 29 avg loss: 0.728439432879289\n",
            "Epoch 30 avg loss: 0.7284395198027293\n",
            "Epoch 31 avg loss: 0.7284395520885786\n",
            "Epoch 32 avg loss: 0.7284393931428591\n",
            "Epoch 33 avg loss: 0.728439673781395\n",
            "Epoch 34 avg loss: 0.7284395297368368\n",
            "Epoch 35 avg loss: 0.7284393534064293\n",
            "Epoch 36 avg loss: 0.7284395222862562\n",
            "Epoch 37 avg loss: 0.7284394279122353\n",
            "Epoch 38 avg loss: 0.7284394179781278\n",
            "Epoch 39 avg loss: 0.7284395123521487\n",
            "Epoch 40 avg loss: 0.72843915472428\n",
            "Epoch 41 avg loss: 0.728439579407374\n",
            "Epoch 42 avg loss: 0.7284395272533098\n",
            "Epoch 43 avg loss: 0.7284396390120188\n",
            "Epoch 44 avg loss: 0.7284395520885786\n",
            "Epoch 45 avg loss: 0.7284393881758054\n",
            "Epoch 46 avg loss: 0.7284394850333532\n",
            "Epoch 47 avg loss: 0.7284397060672442\n",
            "Epoch 48 avg loss: 0.7284394900004069\n",
            "Epoch 49 avg loss: 0.7284397631883621\n",
            "Test:\n",
            "    AP: 0.350 \n",
            "    ROC: 0.511\n",
            "    Avg loss: 0.6503902077674866\n",
            "--------------BIG EPOCH 19--------------\n",
            "Epoch 0 avg loss: 0.7284395943085352\n",
            "Epoch 1 avg loss: 0.7284395943085352\n",
            "Epoch 2 avg loss: 0.7284396092096964\n",
            "Epoch 3 avg loss: 0.7284396116932234\n",
            "Epoch 4 avg loss: 0.728439544637998\n",
            "Epoch 5 avg loss: 0.7284395496050516\n",
            "Epoch 6 avg loss: 0.7284394154946009\n",
            "Epoch 7 avg loss: 0.7284394949674606\n",
            "Epoch 8 avg loss: 0.7284395024180412\n",
            "Epoch 9 avg loss: 0.7284396563967069\n",
            "Epoch 10 avg loss: 0.7284395371874174\n",
            "Epoch 11 avg loss: 0.7284395148356756\n",
            "Epoch 12 avg loss: 0.7284394726157188\n",
            "Epoch 13 avg loss: 0.7284395645062128\n",
            "Epoch 14 avg loss: 0.7284394005934397\n",
            "Epoch 15 avg loss: 0.7284395198027293\n",
            "Epoch 16 avg loss: 0.7284396986166636\n",
            "Epoch 17 avg loss: 0.7284396042426428\n",
            "Epoch 18 avg loss: 0.7284394726157188\n",
            "Epoch 19 avg loss: 0.728439562022686\n",
            "Epoch 20 avg loss: 0.728439599275589\n",
            "Epoch 21 avg loss: 0.7284394577145576\n",
            "Epoch 22 avg loss: 0.7284395347038904\n",
            "Epoch 23 avg loss: 0.7284395645062128\n",
            "Epoch 24 avg loss: 0.7284395943085352\n",
            "Epoch 25 avg loss: 0.7284397309025129\n",
            "Epoch 26 avg loss: 0.7284396390120188\n",
            "Epoch 27 avg loss: 0.7284395272533098\n",
            "Epoch 28 avg loss: 0.7284396092096964\n",
            "Epoch 29 avg loss: 0.7284395967920622\n",
            "Epoch 30 avg loss: 0.7284395893414816\n",
            "Epoch 31 avg loss: 0.7284394949674606\n",
            "Epoch 32 avg loss: 0.7284395049015681\n",
            "Epoch 33 avg loss: 0.7284396439790726\n",
            "Epoch 34 avg loss: 0.7284395570556322\n",
            "Epoch 35 avg loss: 0.7284395198027293\n",
            "Epoch 36 avg loss: 0.728439581890901\n",
            "Epoch 37 avg loss: 0.7284394601980845\n",
            "Epoch 38 avg loss: 0.7284394775827726\n",
            "Epoch 39 avg loss: 0.7284394254287084\n",
            "Epoch 40 avg loss: 0.7284396092096964\n",
            "Epoch 41 avg loss: 0.7284394999345144\n",
            "Epoch 42 avg loss: 0.7284395024180412\n",
            "Epoch 43 avg loss: 0.7284395347038904\n",
            "Epoch 44 avg loss: 0.7284395272533098\n",
            "Epoch 45 avg loss: 0.7284395322203636\n",
            "Epoch 46 avg loss: 0.7284394900004069\n",
            "Epoch 47 avg loss: 0.7284394999345144\n",
            "Epoch 48 avg loss: 0.7284396812319756\n",
            "Epoch 49 avg loss: 0.7284394353628159\n",
            "Test:\n",
            "    AP: 0.369 \n",
            "    ROC: 0.550\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 20--------------\n",
            "Epoch 0 avg loss: 0.7284394279122353\n",
            "Epoch 1 avg loss: 0.7284394924839338\n",
            "Epoch 2 avg loss: 0.7284395322203636\n",
            "Epoch 3 avg loss: 0.7284394452969233\n",
            "Epoch 4 avg loss: 0.728439544637998\n",
            "Epoch 5 avg loss: 0.7284394378463427\n",
            "Epoch 6 avg loss: 0.7284395868579546\n",
            "Epoch 7 avg loss: 0.7284395049015681\n",
            "Epoch 8 avg loss: 0.7284395868579546\n",
            "Epoch 9 avg loss: 0.7284396017591158\n",
            "Epoch 10 avg loss: 0.7284395396709442\n",
            "Epoch 11 avg loss: 0.7284394726157188\n",
            "Epoch 12 avg loss: 0.7284394800662994\n",
            "Epoch 13 avg loss: 0.728439524769783\n",
            "Epoch 14 avg loss: 0.7284394850333532\n",
            "Epoch 15 avg loss: 0.7284393658240637\n",
            "Epoch 16 avg loss: 0.7284394825498263\n",
            "Epoch 17 avg loss: 0.7284395322203636\n",
            "Epoch 18 avg loss: 0.728439562022686\n",
            "Epoch 19 avg loss: 0.7284395868579546\n",
            "Epoch 20 avg loss: 0.7284396241108576\n",
            "Epoch 21 avg loss: 0.7284395868579546\n",
            "Epoch 22 avg loss: 0.7284394651651382\n",
            "Epoch 23 avg loss: 0.7284395297368368\n",
            "Epoch 24 avg loss: 0.7284394279122353\n",
            "Epoch 25 avg loss: 0.7284394428133965\n",
            "Epoch 26 avg loss: 0.7284394130110741\n",
            "Epoch 27 avg loss: 0.7284396042426428\n",
            "Epoch 28 avg loss: 0.7284396390120188\n",
            "Epoch 29 avg loss: 0.7284395843744278\n",
            "Epoch 30 avg loss: 0.7284397184848785\n",
            "Epoch 31 avg loss: 0.7284395173192024\n",
            "Epoch 32 avg loss: 0.728439579407374\n",
            "Epoch 33 avg loss: 0.7284395645062128\n",
            "Epoch 34 avg loss: 0.7284395496050516\n",
            "Epoch 35 avg loss: 0.7284394775827726\n",
            "Epoch 36 avg loss: 0.7284395967920622\n",
            "Epoch 37 avg loss: 0.7284394726157188\n",
            "Epoch 38 avg loss: 0.728439673781395\n",
            "Epoch 39 avg loss: 0.7284396340449651\n",
            "Epoch 40 avg loss: 0.7284395173192024\n",
            "Epoch 41 avg loss: 0.7284395371874174\n",
            "Epoch 42 avg loss: 0.7284395744403204\n",
            "Epoch 43 avg loss: 0.728439544637998\n",
            "Epoch 44 avg loss: 0.7284394577145576\n",
            "Epoch 45 avg loss: 0.728439599275589\n",
            "Epoch 46 avg loss: 0.7284396861990293\n",
            "Epoch 47 avg loss: 0.72843948751688\n",
            "Epoch 48 avg loss: 0.7284395570556322\n",
            "Epoch 49 avg loss: 0.7284394577145576\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.498\n",
            "    Avg loss: 0.6503901481628418\n",
            "--------------BIG EPOCH 21--------------\n",
            "Epoch 0 avg loss: 0.7284394552310308\n",
            "Epoch 1 avg loss: 0.7284395297368368\n",
            "Epoch 2 avg loss: 0.7284395123521487\n",
            "Epoch 3 avg loss: 0.72843965391318\n",
            "Epoch 4 avg loss: 0.7284398376941681\n",
            "Epoch 5 avg loss: 0.7284397160013517\n",
            "Epoch 6 avg loss: 0.7284395123521487\n",
            "Epoch 7 avg loss: 0.7284395347038904\n",
            "Epoch 8 avg loss: 0.7284395347038904\n",
            "Epoch 9 avg loss: 0.72843965391318\n",
            "Epoch 10 avg loss: 0.7284395545721054\n",
            "Epoch 11 avg loss: 0.7284395322203636\n",
            "Epoch 12 avg loss: 0.7284394924839338\n",
            "Epoch 13 avg loss: 0.7284396961331367\n",
            "Epoch 14 avg loss: 0.7284394154946009\n",
            "Epoch 15 avg loss: 0.7284395967920622\n",
            "Epoch 16 avg loss: 0.7284395669897398\n",
            "Epoch 17 avg loss: 0.7284394154946009\n",
            "Epoch 18 avg loss: 0.728439524769783\n",
            "Epoch 19 avg loss: 0.7284394825498263\n",
            "Epoch 20 avg loss: 0.7284395545721054\n",
            "Epoch 21 avg loss: 0.7284394626816114\n",
            "Epoch 22 avg loss: 0.7284395719567934\n",
            "Epoch 23 avg loss: 0.7284395918250084\n",
            "Epoch 24 avg loss: 0.7284395098686218\n",
            "Epoch 25 avg loss: 0.7284396290779114\n",
            "Epoch 26 avg loss: 0.728439507385095\n",
            "Epoch 27 avg loss: 0.728439524769783\n",
            "Epoch 28 avg loss: 0.7284395322203636\n",
            "Epoch 29 avg loss: 0.7284394726157188\n",
            "Epoch 30 avg loss: 0.7284395049015681\n",
            "Epoch 31 avg loss: 0.7284395744403204\n",
            "Epoch 32 avg loss: 0.7284395148356756\n",
            "Epoch 33 avg loss: 0.7284394999345144\n",
            "Epoch 34 avg loss: 0.7284396092096964\n",
            "Epoch 35 avg loss: 0.7284394775827726\n",
            "Epoch 36 avg loss: 0.7284396588802338\n",
            "Epoch 37 avg loss: 0.72843948751688\n",
            "Epoch 38 avg loss: 0.7284395371874174\n",
            "Epoch 39 avg loss: 0.7284396787484487\n",
            "Epoch 40 avg loss: 0.7284396638472875\n",
            "Epoch 41 avg loss: 0.7284395570556322\n",
            "Epoch 42 avg loss: 0.7284395322203636\n",
            "Epoch 43 avg loss: 0.72843965391318\n",
            "Epoch 44 avg loss: 0.7284394949674606\n",
            "Epoch 45 avg loss: 0.7284395719567934\n",
            "Epoch 46 avg loss: 0.7284395148356756\n",
            "Epoch 47 avg loss: 0.7284394825498263\n",
            "Epoch 48 avg loss: 0.7284395520885786\n",
            "Epoch 49 avg loss: 0.728439507385095\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.492\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 22--------------\n",
            "Epoch 0 avg loss: 0.7284396216273308\n",
            "Epoch 1 avg loss: 0.7284395520885786\n",
            "Epoch 2 avg loss: 0.7284394055604935\n",
            "Epoch 3 avg loss: 0.7284394030769666\n",
            "Epoch 4 avg loss: 0.7284394750992457\n",
            "Epoch 5 avg loss: 0.7284395371874174\n",
            "Epoch 6 avg loss: 0.7284395570556322\n",
            "Epoch 7 avg loss: 0.7284395918250084\n",
            "Epoch 8 avg loss: 0.7284396663308144\n",
            "Epoch 9 avg loss: 0.728439581890901\n",
            "Epoch 10 avg loss: 0.7284395694732666\n",
            "Epoch 11 avg loss: 0.7284395918250084\n",
            "Epoch 12 avg loss: 0.7284395396709442\n",
            "Epoch 13 avg loss: 0.7284394999345144\n",
            "Epoch 14 avg loss: 0.7284395347038904\n",
            "Epoch 15 avg loss: 0.728439524769783\n",
            "Epoch 16 avg loss: 0.7284394179781278\n",
            "Epoch 17 avg loss: 0.7284395173192024\n",
            "Epoch 18 avg loss: 0.7284397160013517\n",
            "Epoch 19 avg loss: 0.728439470132192\n",
            "Epoch 20 avg loss: 0.7284395918250084\n",
            "Epoch 21 avg loss: 0.7284394577145576\n",
            "Epoch 22 avg loss: 0.7284396414955457\n",
            "Epoch 23 avg loss: 0.7284395496050516\n",
            "Epoch 24 avg loss: 0.7284395471215248\n",
            "Epoch 25 avg loss: 0.7284396712978681\n",
            "Epoch 26 avg loss: 0.7284396116932234\n",
            "Epoch 27 avg loss: 0.7284393906593323\n",
            "Epoch 28 avg loss: 0.7284395719567934\n",
            "Epoch 29 avg loss: 0.7284395893414816\n",
            "Epoch 30 avg loss: 0.7284395769238472\n",
            "Epoch 31 avg loss: 0.7284395893414816\n",
            "Epoch 32 avg loss: 0.7284396588802338\n",
            "Epoch 33 avg loss: 0.7284394949674606\n",
            "Epoch 34 avg loss: 0.7284395769238472\n",
            "Epoch 35 avg loss: 0.7284394353628159\n",
            "Epoch 36 avg loss: 0.7284394949674606\n",
            "Epoch 37 avg loss: 0.7284394750992457\n",
            "Epoch 38 avg loss: 0.7284394601980845\n",
            "Epoch 39 avg loss: 0.7284396241108576\n",
            "Epoch 40 avg loss: 0.7284396464625994\n",
            "Epoch 41 avg loss: 0.7284394527475039\n",
            "Epoch 42 avg loss: 0.7284398327271143\n",
            "Epoch 43 avg loss: 0.7284396340449651\n",
            "Epoch 44 avg loss: 0.7284396390120188\n",
            "Epoch 45 avg loss: 0.728439432879289\n",
            "Epoch 46 avg loss: 0.7284394651651382\n",
            "Epoch 47 avg loss: 0.7284394005934397\n",
            "Epoch 48 avg loss: 0.7284395297368368\n",
            "Epoch 49 avg loss: 0.7284396017591158\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503897905349731\n",
            "--------------BIG EPOCH 23--------------\n",
            "Epoch 0 avg loss: 0.7284394750992457\n",
            "Epoch 1 avg loss: 0.7284395222862562\n",
            "Epoch 2 avg loss: 0.728439544637998\n",
            "Epoch 3 avg loss: 0.7284394750992457\n",
            "Epoch 4 avg loss: 0.728439542154471\n",
            "Epoch 5 avg loss: 0.7284395918250084\n",
            "Epoch 6 avg loss: 0.7284394999345144\n",
            "Epoch 7 avg loss: 0.7284395471215248\n",
            "Epoch 8 avg loss: 0.7284395148356756\n",
            "Epoch 9 avg loss: 0.7284396042426428\n",
            "Epoch 10 avg loss: 0.728439395626386\n",
            "Epoch 11 avg loss: 0.7284397532542547\n",
            "Epoch 12 avg loss: 0.7284395694732666\n",
            "Epoch 13 avg loss: 0.7284395272533098\n",
            "Epoch 14 avg loss: 0.7284395570556322\n",
            "Epoch 15 avg loss: 0.728439507385095\n",
            "Epoch 16 avg loss: 0.7284393409887949\n",
            "Epoch 17 avg loss: 0.7284394452969233\n",
            "Epoch 18 avg loss: 0.7284397135178248\n",
            "Epoch 19 avg loss: 0.7284395272533098\n",
            "Epoch 20 avg loss: 0.728439507385095\n",
            "Epoch 21 avg loss: 0.72843965391318\n",
            "Epoch 22 avg loss: 0.7284396216273308\n",
            "Epoch 23 avg loss: 0.7284396514296532\n",
            "Epoch 24 avg loss: 0.7284393981099129\n",
            "Epoch 25 avg loss: 0.7284394726157188\n",
            "Epoch 26 avg loss: 0.7284394080440203\n",
            "Epoch 27 avg loss: 0.7284394775827726\n",
            "Epoch 28 avg loss: 0.7284394477804502\n",
            "Epoch 29 avg loss: 0.7284396464625994\n",
            "Epoch 30 avg loss: 0.7284395272533098\n",
            "Epoch 31 avg loss: 0.7284395198027293\n",
            "Epoch 32 avg loss: 0.7284395943085352\n",
            "Epoch 33 avg loss: 0.7284395496050516\n",
            "Epoch 34 avg loss: 0.7284394850333532\n",
            "Epoch 35 avg loss: 0.7284396017591158\n",
            "Epoch 36 avg loss: 0.7284394428133965\n",
            "Epoch 37 avg loss: 0.7284395967920622\n",
            "Epoch 38 avg loss: 0.7284394154946009\n",
            "Epoch 39 avg loss: 0.7284395943085352\n",
            "Epoch 40 avg loss: 0.7284393732746443\n",
            "Epoch 41 avg loss: 0.7284396414955457\n",
            "Epoch 42 avg loss: 0.7284394800662994\n",
            "Epoch 43 avg loss: 0.7284393583734831\n",
            "Epoch 44 avg loss: 0.7284394601980845\n",
            "Epoch 45 avg loss: 0.7284395719567934\n",
            "Epoch 46 avg loss: 0.7284394254287084\n",
            "Epoch 47 avg loss: 0.728439450263977\n",
            "Epoch 48 avg loss: 0.7284395893414816\n",
            "Epoch 49 avg loss: 0.728439450263977\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.650390088558197\n",
            "--------------BIG EPOCH 24--------------\n",
            "Epoch 0 avg loss: 0.7284395396709442\n",
            "Epoch 1 avg loss: 0.7284393484393755\n",
            "Epoch 2 avg loss: 0.7284395943085352\n",
            "Epoch 3 avg loss: 0.7284395943085352\n",
            "Epoch 4 avg loss: 0.7284395496050516\n",
            "Epoch 5 avg loss: 0.7284395744403204\n",
            "Epoch 6 avg loss: 0.7284395645062128\n",
            "Epoch 7 avg loss: 0.7284396638472875\n",
            "Epoch 8 avg loss: 0.7284394477804502\n",
            "Epoch 9 avg loss: 0.7284395297368368\n",
            "Epoch 10 avg loss: 0.7284395868579546\n",
            "Epoch 11 avg loss: 0.7284394179781278\n",
            "Epoch 12 avg loss: 0.7284395868579546\n",
            "Epoch 13 avg loss: 0.7284397160013517\n",
            "Epoch 14 avg loss: 0.7284397333860397\n",
            "Epoch 15 avg loss: 0.7284394825498263\n",
            "Epoch 16 avg loss: 0.7284394527475039\n",
            "Epoch 17 avg loss: 0.7284395520885786\n",
            "Epoch 18 avg loss: 0.7284394577145576\n",
            "Epoch 19 avg loss: 0.7284395868579546\n",
            "Epoch 20 avg loss: 0.7284394055604935\n",
            "Epoch 21 avg loss: 0.728439507385095\n",
            "Epoch 22 avg loss: 0.7284394279122353\n",
            "Epoch 23 avg loss: 0.7284394452969233\n",
            "Epoch 24 avg loss: 0.7284395148356756\n",
            "Epoch 25 avg loss: 0.7284395198027293\n",
            "Epoch 26 avg loss: 0.7284395148356756\n",
            "Epoch 27 avg loss: 0.7284396837155024\n",
            "Epoch 28 avg loss: 0.7284394726157188\n",
            "Epoch 29 avg loss: 0.7284394651651382\n",
            "Epoch 30 avg loss: 0.7284394378463427\n",
            "Epoch 31 avg loss: 0.728439636528492\n",
            "Epoch 32 avg loss: 0.7284395967920622\n",
            "Epoch 33 avg loss: 0.7284396439790726\n",
            "Epoch 34 avg loss: 0.7284395098686218\n",
            "Epoch 35 avg loss: 0.7284395893414816\n",
            "Epoch 36 avg loss: 0.7284395967920622\n",
            "Epoch 37 avg loss: 0.7284395173192024\n",
            "Epoch 38 avg loss: 0.728439432879289\n",
            "Epoch 39 avg loss: 0.728439507385095\n",
            "Epoch 40 avg loss: 0.728439579407374\n",
            "Epoch 41 avg loss: 0.7284395918250084\n",
            "Epoch 42 avg loss: 0.7284394949674606\n",
            "Epoch 43 avg loss: 0.7284395222862562\n",
            "Epoch 44 avg loss: 0.728439579407374\n",
            "Epoch 45 avg loss: 0.7284395694732666\n",
            "Epoch 46 avg loss: 0.7284395694732666\n",
            "Epoch 47 avg loss: 0.728439691166083\n",
            "Epoch 48 avg loss: 0.7284395595391592\n",
            "Epoch 49 avg loss: 0.7284395222862562\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 25--------------\n",
            "Epoch 0 avg loss: 0.7284393558899561\n",
            "Epoch 1 avg loss: 0.7284396688143412\n",
            "Epoch 2 avg loss: 0.728439581890901\n",
            "Epoch 3 avg loss: 0.7284396762649218\n",
            "Epoch 4 avg loss: 0.7284395322203636\n",
            "Epoch 5 avg loss: 0.7284394974509875\n",
            "Epoch 6 avg loss: 0.7284395769238472\n",
            "Epoch 7 avg loss: 0.7284395868579546\n",
            "Epoch 8 avg loss: 0.728439432879289\n",
            "Epoch 9 avg loss: 0.7284394452969233\n",
            "Epoch 10 avg loss: 0.7284395669897398\n",
            "Epoch 11 avg loss: 0.7284395496050516\n",
            "Epoch 12 avg loss: 0.7284395744403204\n",
            "Epoch 13 avg loss: 0.7284395471215248\n",
            "Epoch 14 avg loss: 0.7284395694732666\n",
            "Epoch 15 avg loss: 0.7284395868579546\n",
            "Epoch 16 avg loss: 0.7284395545721054\n",
            "Epoch 17 avg loss: 0.7284395520885786\n",
            "Epoch 18 avg loss: 0.7284396315614382\n",
            "Epoch 19 avg loss: 0.728439599275589\n",
            "Epoch 20 avg loss: 0.7284395347038904\n",
            "Epoch 21 avg loss: 0.7284395967920622\n",
            "Epoch 22 avg loss: 0.7284396961331367\n",
            "Epoch 23 avg loss: 0.7284395669897398\n",
            "Epoch 24 avg loss: 0.7284394750992457\n",
            "Epoch 25 avg loss: 0.728439616660277\n",
            "Epoch 26 avg loss: 0.7284394601980845\n",
            "Epoch 27 avg loss: 0.7284394924839338\n",
            "Epoch 28 avg loss: 0.7284393881758054\n",
            "Epoch 29 avg loss: 0.7284395769238472\n",
            "Epoch 30 avg loss: 0.7284394552310308\n",
            "Epoch 31 avg loss: 0.7284394949674606\n",
            "Epoch 32 avg loss: 0.7284396563967069\n",
            "Epoch 33 avg loss: 0.7284396464625994\n",
            "Epoch 34 avg loss: 0.7284394080440203\n",
            "Epoch 35 avg loss: 0.7284394775827726\n",
            "Epoch 36 avg loss: 0.7284394850333532\n",
            "Epoch 37 avg loss: 0.72843948751688\n",
            "Epoch 38 avg loss: 0.7284394726157188\n",
            "Epoch 39 avg loss: 0.7284396017591158\n",
            "Epoch 40 avg loss: 0.7284395371874174\n",
            "Epoch 41 avg loss: 0.7284396886825562\n",
            "Epoch 42 avg loss: 0.7284394800662994\n",
            "Epoch 43 avg loss: 0.7284396688143412\n",
            "Epoch 44 avg loss: 0.7284394378463427\n",
            "Epoch 45 avg loss: 0.7284395645062128\n",
            "Epoch 46 avg loss: 0.7284395744403204\n",
            "Epoch 47 avg loss: 0.7284395173192024\n",
            "Epoch 48 avg loss: 0.7284395967920622\n",
            "Epoch 49 avg loss: 0.7284395769238472\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503894329071045\n",
            "--------------BIG EPOCH 26--------------\n",
            "Epoch 0 avg loss: 0.7284395967920622\n",
            "Epoch 1 avg loss: 0.7284395570556322\n",
            "Epoch 2 avg loss: 0.7284396241108576\n",
            "Epoch 3 avg loss: 0.7284396390120188\n",
            "Epoch 4 avg loss: 0.7284395173192024\n",
            "Epoch 5 avg loss: 0.7284395769238472\n",
            "Epoch 6 avg loss: 0.7284394452969233\n",
            "Epoch 7 avg loss: 0.7284393881758054\n",
            "Epoch 8 avg loss: 0.7284396638472875\n",
            "Epoch 9 avg loss: 0.728439524769783\n",
            "Epoch 10 avg loss: 0.7284397184848785\n",
            "Epoch 11 avg loss: 0.7284394577145576\n",
            "Epoch 12 avg loss: 0.7284394949674606\n",
            "Epoch 13 avg loss: 0.728439542154471\n",
            "Epoch 14 avg loss: 0.7284395967920622\n",
            "Epoch 15 avg loss: 0.7284395595391592\n",
            "Epoch 16 avg loss: 0.7284395396709442\n",
            "Epoch 17 avg loss: 0.7284394204616547\n",
            "Epoch 18 avg loss: 0.7284395967920622\n",
            "Epoch 19 avg loss: 0.7284395769238472\n",
            "Epoch 20 avg loss: 0.728439579407374\n",
            "Epoch 21 avg loss: 0.7284394775827726\n",
            "Epoch 22 avg loss: 0.7284396042426428\n",
            "Epoch 23 avg loss: 0.7284395347038904\n",
            "Epoch 24 avg loss: 0.7284396563967069\n",
            "Epoch 25 avg loss: 0.7284395868579546\n",
            "Epoch 26 avg loss: 0.7284394999345144\n",
            "Epoch 27 avg loss: 0.7284395520885786\n",
            "Epoch 28 avg loss: 0.72843948751688\n",
            "Epoch 29 avg loss: 0.7284394601980845\n",
            "Epoch 30 avg loss: 0.7284395297368368\n",
            "Epoch 31 avg loss: 0.7284396216273308\n",
            "Epoch 32 avg loss: 0.7284394378463427\n",
            "Epoch 33 avg loss: 0.7284396563967069\n",
            "Epoch 34 avg loss: 0.728439599275589\n",
            "Epoch 35 avg loss: 0.7284395843744278\n",
            "Epoch 36 avg loss: 0.7284395744403204\n",
            "Epoch 37 avg loss: 0.7284395645062128\n",
            "Epoch 38 avg loss: 0.7284395719567934\n",
            "Epoch 39 avg loss: 0.7284394825498263\n",
            "Epoch 40 avg loss: 0.7284395744403204\n",
            "Epoch 41 avg loss: 0.7284394577145576\n",
            "Epoch 42 avg loss: 0.7284396613637606\n",
            "Epoch 43 avg loss: 0.7284394601980845\n",
            "Epoch 44 avg loss: 0.7284394080440203\n",
            "Epoch 45 avg loss: 0.7284394750992457\n",
            "Epoch 46 avg loss: 0.7284395967920622\n",
            "Epoch 47 avg loss: 0.7284395396709442\n",
            "Epoch 48 avg loss: 0.7284395769238472\n",
            "Epoch 49 avg loss: 0.7284394750992457\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503899693489075\n",
            "--------------BIG EPOCH 27--------------\n",
            "Epoch 0 avg loss: 0.7284394577145576\n",
            "Epoch 1 avg loss: 0.728439544637998\n",
            "Epoch 2 avg loss: 0.7284395173192024\n",
            "Epoch 3 avg loss: 0.7284396961331367\n",
            "Epoch 4 avg loss: 0.728439544637998\n",
            "Epoch 5 avg loss: 0.7284395694732666\n",
            "Epoch 6 avg loss: 0.7284396588802338\n",
            "Epoch 7 avg loss: 0.7284396092096964\n",
            "Epoch 8 avg loss: 0.7284394527475039\n",
            "Epoch 9 avg loss: 0.72843948751688\n",
            "Epoch 10 avg loss: 0.7284395893414816\n",
            "Epoch 11 avg loss: 0.7284395148356756\n",
            "Epoch 12 avg loss: 0.7284395371874174\n",
            "Epoch 13 avg loss: 0.7284395744403204\n",
            "Epoch 14 avg loss: 0.7284395769238472\n",
            "Epoch 15 avg loss: 0.7284394651651382\n",
            "Epoch 16 avg loss: 0.7284396017591158\n",
            "Epoch 17 avg loss: 0.7284395669897398\n",
            "Epoch 18 avg loss: 0.7284395520885786\n",
            "Epoch 19 avg loss: 0.7284397184848785\n",
            "Epoch 20 avg loss: 0.728439636528492\n",
            "Epoch 21 avg loss: 0.7284395123521487\n",
            "Epoch 22 avg loss: 0.7284394080440203\n",
            "Epoch 23 avg loss: 0.728439470132192\n",
            "Epoch 24 avg loss: 0.7284395148356756\n",
            "Epoch 25 avg loss: 0.7284394924839338\n",
            "Epoch 26 avg loss: 0.728439432879289\n",
            "Epoch 27 avg loss: 0.7284393260876337\n",
            "Epoch 28 avg loss: 0.7284394726157188\n",
            "Epoch 29 avg loss: 0.7284397160013517\n",
            "Epoch 30 avg loss: 0.7284395198027293\n",
            "Epoch 31 avg loss: 0.7284396663308144\n",
            "Epoch 32 avg loss: 0.728439636528492\n",
            "Epoch 33 avg loss: 0.7284396067261696\n",
            "Epoch 34 avg loss: 0.728439619143804\n",
            "Epoch 35 avg loss: 0.7284396216273308\n",
            "Epoch 36 avg loss: 0.7284395322203636\n",
            "Epoch 37 avg loss: 0.7284394428133965\n",
            "Epoch 38 avg loss: 0.7284394850333532\n",
            "Epoch 39 avg loss: 0.728439579407374\n",
            "Epoch 40 avg loss: 0.728439524769783\n",
            "Epoch 41 avg loss: 0.7284395123521487\n",
            "Epoch 42 avg loss: 0.7284394254287084\n",
            "Epoch 43 avg loss: 0.7284394999345144\n",
            "Epoch 44 avg loss: 0.7284394800662994\n",
            "Epoch 45 avg loss: 0.7284395024180412\n",
            "Epoch 46 avg loss: 0.7284396067261696\n",
            "Epoch 47 avg loss: 0.7284396464625994\n",
            "Epoch 48 avg loss: 0.7284395769238472\n",
            "Epoch 49 avg loss: 0.7284394303957621\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503900289535522\n",
            "--------------BIG EPOCH 28--------------\n",
            "Epoch 0 avg loss: 0.7284395943085352\n",
            "Epoch 1 avg loss: 0.728439562022686\n",
            "Epoch 2 avg loss: 0.7284395893414816\n",
            "Epoch 3 avg loss: 0.7284396390120188\n",
            "Epoch 4 avg loss: 0.728439524769783\n",
            "Epoch 5 avg loss: 0.7284395669897398\n",
            "Epoch 6 avg loss: 0.7284394353628159\n",
            "Epoch 7 avg loss: 0.7284395595391592\n",
            "Epoch 8 avg loss: 0.7284396290779114\n",
            "Epoch 9 avg loss: 0.728439524769783\n",
            "Epoch 10 avg loss: 0.7284396092096964\n",
            "Epoch 11 avg loss: 0.7284396588802338\n",
            "Epoch 12 avg loss: 0.7284394924839338\n",
            "Epoch 13 avg loss: 0.7284393707911173\n",
            "Epoch 14 avg loss: 0.7284394477804502\n",
            "Epoch 15 avg loss: 0.7284395297368368\n",
            "Epoch 16 avg loss: 0.7284395843744278\n",
            "Epoch 17 avg loss: 0.728439450263977\n",
            "Epoch 18 avg loss: 0.7284395744403204\n",
            "Epoch 19 avg loss: 0.728439544637998\n",
            "Epoch 20 avg loss: 0.7284394825498263\n",
            "Epoch 21 avg loss: 0.7284394825498263\n",
            "Epoch 22 avg loss: 0.7284395918250084\n",
            "Epoch 23 avg loss: 0.7284396092096964\n",
            "Epoch 24 avg loss: 0.728439579407374\n",
            "Epoch 25 avg loss: 0.7284394527475039\n",
            "Epoch 26 avg loss: 0.7284395645062128\n",
            "Epoch 27 avg loss: 0.7284396812319756\n",
            "Epoch 28 avg loss: 0.7284397532542547\n",
            "Epoch 29 avg loss: 0.7284394403298696\n",
            "Epoch 30 avg loss: 0.728439544637998\n",
            "Epoch 31 avg loss: 0.7284394428133965\n",
            "Epoch 32 avg loss: 0.7284395843744278\n",
            "Epoch 33 avg loss: 0.728439507385095\n",
            "Epoch 34 avg loss: 0.7284396340449651\n",
            "Epoch 35 avg loss: 0.7284394900004069\n",
            "Epoch 36 avg loss: 0.7284395322203636\n",
            "Epoch 37 avg loss: 0.7284395098686218\n",
            "Epoch 38 avg loss: 0.7284395893414816\n",
            "Epoch 39 avg loss: 0.728439673781395\n",
            "Epoch 40 avg loss: 0.7284394850333532\n",
            "Epoch 41 avg loss: 0.7284394726157188\n",
            "Epoch 42 avg loss: 0.7284395496050516\n",
            "Epoch 43 avg loss: 0.7284394130110741\n",
            "Epoch 44 avg loss: 0.7284393906593323\n",
            "Epoch 45 avg loss: 0.7284396390120188\n",
            "Epoch 46 avg loss: 0.7284394726157188\n",
            "Epoch 47 avg loss: 0.728439673781395\n",
            "Epoch 48 avg loss: 0.7284396464625994\n",
            "Epoch 49 avg loss: 0.728439691166083\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503898501396179\n",
            "--------------BIG EPOCH 29--------------\n",
            "Epoch 0 avg loss: 0.7284395322203636\n",
            "Epoch 1 avg loss: 0.7284394403298696\n",
            "Epoch 2 avg loss: 0.7284395967920622\n",
            "Epoch 3 avg loss: 0.7284396613637606\n",
            "Epoch 4 avg loss: 0.7284396514296532\n",
            "Epoch 5 avg loss: 0.7284395744403204\n",
            "Epoch 6 avg loss: 0.728439432879289\n",
            "Epoch 7 avg loss: 0.7284395843744278\n",
            "Epoch 8 avg loss: 0.7284396116932234\n",
            "Epoch 9 avg loss: 0.7284395272533098\n",
            "Epoch 10 avg loss: 0.7284394726157188\n",
            "Epoch 11 avg loss: 0.7284395893414816\n",
            "Epoch 12 avg loss: 0.7284395496050516\n",
            "Epoch 13 avg loss: 0.7284395520885786\n",
            "Epoch 14 avg loss: 0.7284394800662994\n",
            "Epoch 15 avg loss: 0.7284394825498263\n",
            "Epoch 16 avg loss: 0.7284396464625994\n",
            "Epoch 17 avg loss: 0.7284394229451815\n",
            "Epoch 18 avg loss: 0.72843948751688\n",
            "Epoch 19 avg loss: 0.7284397234519323\n",
            "Epoch 20 avg loss: 0.7284394204616547\n",
            "Epoch 21 avg loss: 0.7284394626816114\n",
            "Epoch 22 avg loss: 0.7284396762649218\n",
            "Epoch 23 avg loss: 0.7284397160013517\n",
            "Epoch 24 avg loss: 0.728439636528492\n",
            "Epoch 25 avg loss: 0.7284395843744278\n",
            "Epoch 26 avg loss: 0.7284395496050516\n",
            "Epoch 27 avg loss: 0.7284395123521487\n",
            "Epoch 28 avg loss: 0.7284393136699995\n",
            "Epoch 29 avg loss: 0.7284394254287084\n",
            "Epoch 30 avg loss: 0.7284396017591158\n",
            "Epoch 31 avg loss: 0.7284393409887949\n",
            "Epoch 32 avg loss: 0.7284394130110741\n",
            "Epoch 33 avg loss: 0.7284395496050516\n",
            "Epoch 34 avg loss: 0.7284396489461263\n",
            "Epoch 35 avg loss: 0.7284395371874174\n",
            "Epoch 36 avg loss: 0.7284395645062128\n",
            "Epoch 37 avg loss: 0.7284395396709442\n",
            "Epoch 38 avg loss: 0.7284396017591158\n",
            "Epoch 39 avg loss: 0.7284395347038904\n",
            "Epoch 40 avg loss: 0.7284395943085352\n",
            "Epoch 41 avg loss: 0.7284395694732666\n",
            "Epoch 42 avg loss: 0.7284395843744278\n",
            "Epoch 43 avg loss: 0.72843948751688\n",
            "Epoch 44 avg loss: 0.7284394452969233\n",
            "Epoch 45 avg loss: 0.7284395173192024\n",
            "Epoch 46 avg loss: 0.7284396141767502\n",
            "Epoch 47 avg loss: 0.7284394900004069\n",
            "Epoch 48 avg loss: 0.7284394005934397\n",
            "Epoch 49 avg loss: 0.7284394726157188\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.474\n",
            "    Avg loss: 0.6503894329071045\n",
            "--------------BIG EPOCH 30--------------\n",
            "Epoch 0 avg loss: 0.7284394924839338\n",
            "Epoch 1 avg loss: 0.7284394825498263\n",
            "Epoch 2 avg loss: 0.7284395570556322\n",
            "Epoch 3 avg loss: 0.72843948751688\n",
            "Epoch 4 avg loss: 0.7284394775827726\n",
            "Epoch 5 avg loss: 0.7284396067261696\n",
            "Epoch 6 avg loss: 0.7284395967920622\n",
            "Epoch 7 avg loss: 0.7284394726157188\n",
            "Epoch 8 avg loss: 0.728439581890901\n",
            "Epoch 9 avg loss: 0.7284396265943845\n",
            "Epoch 10 avg loss: 0.7284394452969233\n",
            "Epoch 11 avg loss: 0.7284394726157188\n",
            "Epoch 12 avg loss: 0.7284395843744278\n",
            "Epoch 13 avg loss: 0.728439616660277\n",
            "Epoch 14 avg loss: 0.7284394974509875\n",
            "Epoch 15 avg loss: 0.7284395396709442\n",
            "Epoch 16 avg loss: 0.7284396712978681\n",
            "Epoch 17 avg loss: 0.7284395098686218\n",
            "Epoch 18 avg loss: 0.7284395545721054\n",
            "Epoch 19 avg loss: 0.7284396638472875\n",
            "Epoch 20 avg loss: 0.7284395496050516\n",
            "Epoch 21 avg loss: 0.7284395371874174\n",
            "Epoch 22 avg loss: 0.728439619143804\n",
            "Epoch 23 avg loss: 0.7284395918250084\n",
            "Epoch 24 avg loss: 0.7284395669897398\n",
            "Epoch 25 avg loss: 0.7284394900004069\n",
            "Epoch 26 avg loss: 0.7284396464625994\n",
            "Epoch 27 avg loss: 0.7284396688143412\n",
            "Epoch 28 avg loss: 0.7284395918250084\n",
            "Epoch 29 avg loss: 0.7284395272533098\n",
            "Epoch 30 avg loss: 0.7284396315614382\n",
            "Epoch 31 avg loss: 0.7284395595391592\n",
            "Epoch 32 avg loss: 0.7284396514296532\n",
            "Epoch 33 avg loss: 0.7284395148356756\n",
            "Epoch 34 avg loss: 0.7284395545721054\n",
            "Epoch 35 avg loss: 0.7284394626816114\n",
            "Epoch 36 avg loss: 0.728439450263977\n",
            "Epoch 37 avg loss: 0.7284395719567934\n",
            "Epoch 38 avg loss: 0.7284395669897398\n",
            "Epoch 39 avg loss: 0.7284396067261696\n",
            "Epoch 40 avg loss: 0.7284394775827726\n",
            "Epoch 41 avg loss: 0.7284395893414816\n",
            "Epoch 42 avg loss: 0.7284395545721054\n",
            "Epoch 43 avg loss: 0.7284395496050516\n",
            "Epoch 44 avg loss: 0.7284393906593323\n",
            "Epoch 45 avg loss: 0.728439507385095\n",
            "Epoch 46 avg loss: 0.7284394800662994\n",
            "Epoch 47 avg loss: 0.7284393757581711\n",
            "Epoch 48 avg loss: 0.7284395098686218\n",
            "Epoch 49 avg loss: 0.7284395645062128\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 31--------------\n",
            "Epoch 0 avg loss: 0.7284394477804502\n",
            "Epoch 1 avg loss: 0.7284395148356756\n",
            "Epoch 2 avg loss: 0.7284396092096964\n",
            "Epoch 3 avg loss: 0.7284394353628159\n",
            "Epoch 4 avg loss: 0.7284395967920622\n",
            "Epoch 5 avg loss: 0.7284395496050516\n",
            "Epoch 6 avg loss: 0.7284395669897398\n",
            "Epoch 7 avg loss: 0.7284395222862562\n",
            "Epoch 8 avg loss: 0.7284394353628159\n",
            "Epoch 9 avg loss: 0.7284395868579546\n",
            "Epoch 10 avg loss: 0.7284396092096964\n",
            "Epoch 11 avg loss: 0.728439542154471\n",
            "Epoch 12 avg loss: 0.7284395744403204\n",
            "Epoch 13 avg loss: 0.7284397011001905\n",
            "Epoch 14 avg loss: 0.7284395570556322\n",
            "Epoch 15 avg loss: 0.7284396464625994\n",
            "Epoch 16 avg loss: 0.7284395371874174\n",
            "Epoch 17 avg loss: 0.7284395371874174\n",
            "Epoch 18 avg loss: 0.7284395222862562\n",
            "Epoch 19 avg loss: 0.7284394800662994\n",
            "Epoch 20 avg loss: 0.7284395049015681\n",
            "Epoch 21 avg loss: 0.7284396563967069\n",
            "Epoch 22 avg loss: 0.7284395843744278\n",
            "Epoch 23 avg loss: 0.728439562022686\n",
            "Epoch 24 avg loss: 0.7284394850333532\n",
            "Epoch 25 avg loss: 0.7284394800662994\n",
            "Epoch 26 avg loss: 0.7284395396709442\n",
            "Epoch 27 avg loss: 0.7284395471215248\n",
            "Epoch 28 avg loss: 0.7284395967920622\n",
            "Epoch 29 avg loss: 0.7284395347038904\n",
            "Epoch 30 avg loss: 0.7284394378463427\n",
            "Epoch 31 avg loss: 0.7284395297368368\n",
            "Epoch 32 avg loss: 0.7284394378463427\n",
            "Epoch 33 avg loss: 0.7284396514296532\n",
            "Epoch 34 avg loss: 0.7284394080440203\n",
            "Epoch 35 avg loss: 0.7284394130110741\n",
            "Epoch 36 avg loss: 0.7284395769238472\n",
            "Epoch 37 avg loss: 0.7284395595391592\n",
            "Epoch 38 avg loss: 0.7284395322203636\n",
            "Epoch 39 avg loss: 0.7284395198027293\n",
            "Epoch 40 avg loss: 0.7284393633405367\n",
            "Epoch 41 avg loss: 0.7284396712978681\n",
            "Epoch 42 avg loss: 0.7284394154946009\n",
            "Epoch 43 avg loss: 0.7284395396709442\n",
            "Epoch 44 avg loss: 0.7284394924839338\n",
            "Epoch 45 avg loss: 0.7284394900004069\n",
            "Epoch 46 avg loss: 0.7284395645062128\n",
            "Epoch 47 avg loss: 0.7284396638472875\n",
            "Epoch 48 avg loss: 0.7284395918250084\n",
            "Epoch 49 avg loss: 0.7284394726157188\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.498\n",
            "    Avg loss: 0.650389552116394\n",
            "--------------BIG EPOCH 32--------------\n",
            "Epoch 0 avg loss: 0.7284395024180412\n",
            "Epoch 1 avg loss: 0.7284393931428591\n",
            "Epoch 2 avg loss: 0.7284394601980845\n",
            "Epoch 3 avg loss: 0.7284397110342979\n",
            "Epoch 4 avg loss: 0.7284396141767502\n",
            "Epoch 5 avg loss: 0.7284396588802338\n",
            "Epoch 6 avg loss: 0.7284394949674606\n",
            "Epoch 7 avg loss: 0.728439562022686\n",
            "Epoch 8 avg loss: 0.7284394403298696\n",
            "Epoch 9 avg loss: 0.728439432879289\n",
            "Epoch 10 avg loss: 0.7284395669897398\n",
            "Epoch 11 avg loss: 0.7284396241108576\n",
            "Epoch 12 avg loss: 0.7284395669897398\n",
            "Epoch 13 avg loss: 0.7284396563967069\n",
            "Epoch 14 avg loss: 0.728439728418986\n",
            "Epoch 15 avg loss: 0.7284396315614382\n",
            "Epoch 16 avg loss: 0.7284395769238472\n",
            "Epoch 17 avg loss: 0.7284394850333532\n",
            "Epoch 18 avg loss: 0.7284396514296532\n",
            "Epoch 19 avg loss: 0.7284395297368368\n",
            "Epoch 20 avg loss: 0.7284394900004069\n",
            "Epoch 21 avg loss: 0.728439507385095\n",
            "Epoch 22 avg loss: 0.7284394974509875\n",
            "Epoch 23 avg loss: 0.7284394577145576\n",
            "Epoch 24 avg loss: 0.728439619143804\n",
            "Epoch 25 avg loss: 0.7284394924839338\n",
            "Epoch 26 avg loss: 0.7284395173192024\n",
            "Epoch 27 avg loss: 0.7284396092096964\n",
            "Epoch 28 avg loss: 0.7284395322203636\n",
            "Epoch 29 avg loss: 0.7284394850333532\n",
            "Epoch 30 avg loss: 0.7284395868579546\n",
            "Epoch 31 avg loss: 0.7284396886825562\n",
            "Epoch 32 avg loss: 0.7284396017591158\n",
            "Epoch 33 avg loss: 0.7284395918250084\n",
            "Epoch 34 avg loss: 0.7284395843744278\n",
            "Epoch 35 avg loss: 0.7284395098686218\n",
            "Epoch 36 avg loss: 0.7284394527475039\n",
            "Epoch 37 avg loss: 0.7284395943085352\n",
            "Epoch 38 avg loss: 0.7284396017591158\n",
            "Epoch 39 avg loss: 0.7284395098686218\n",
            "Epoch 40 avg loss: 0.7284395595391592\n",
            "Epoch 41 avg loss: 0.7284396141767502\n",
            "Epoch 42 avg loss: 0.7284394577145576\n",
            "Epoch 43 avg loss: 0.7284395893414816\n",
            "Epoch 44 avg loss: 0.7284394974509875\n",
            "Epoch 45 avg loss: 0.7284395967920622\n",
            "Epoch 46 avg loss: 0.7284395173192024\n",
            "Epoch 47 avg loss: 0.7284395843744278\n",
            "Epoch 48 avg loss: 0.728439470132192\n",
            "Epoch 49 avg loss: 0.7284395943085352\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503896713256836\n",
            "--------------BIG EPOCH 33--------------\n",
            "Epoch 0 avg loss: 0.7284396340449651\n",
            "Epoch 1 avg loss: 0.7284394924839338\n",
            "Epoch 2 avg loss: 0.728439524769783\n",
            "Epoch 3 avg loss: 0.7284396042426428\n",
            "Epoch 4 avg loss: 0.7284395645062128\n",
            "Epoch 5 avg loss: 0.7284396390120188\n",
            "Epoch 6 avg loss: 0.7284396092096964\n",
            "Epoch 7 avg loss: 0.7284396787484487\n",
            "Epoch 8 avg loss: 0.7284395198027293\n",
            "Epoch 9 avg loss: 0.7284394850333532\n",
            "Epoch 10 avg loss: 0.7284395297368368\n",
            "Epoch 11 avg loss: 0.7284395198027293\n",
            "Epoch 12 avg loss: 0.728439579407374\n",
            "Epoch 13 avg loss: 0.7284394924839338\n",
            "Epoch 14 avg loss: 0.7284394378463427\n",
            "Epoch 15 avg loss: 0.7284394527475039\n",
            "Epoch 16 avg loss: 0.7284395198027293\n",
            "Epoch 17 avg loss: 0.7284394800662994\n",
            "Epoch 18 avg loss: 0.7284394055604935\n",
            "Epoch 19 avg loss: 0.728439544637998\n",
            "Epoch 20 avg loss: 0.728439542154471\n",
            "Epoch 21 avg loss: 0.7284395347038904\n",
            "Epoch 22 avg loss: 0.7284394229451815\n",
            "Epoch 23 avg loss: 0.7284395322203636\n",
            "Epoch 24 avg loss: 0.7284395297368368\n",
            "Epoch 25 avg loss: 0.72843948751688\n",
            "Epoch 26 avg loss: 0.7284394775827726\n",
            "Epoch 27 avg loss: 0.7284395098686218\n",
            "Epoch 28 avg loss: 0.7284395595391592\n",
            "Epoch 29 avg loss: 0.7284395272533098\n",
            "Epoch 30 avg loss: 0.7284395396709442\n",
            "Epoch 31 avg loss: 0.7284395545721054\n",
            "Epoch 32 avg loss: 0.7284395496050516\n",
            "Epoch 33 avg loss: 0.7284397060672442\n",
            "Epoch 34 avg loss: 0.7284395769238472\n",
            "Epoch 35 avg loss: 0.7284396688143412\n",
            "Epoch 36 avg loss: 0.7284395520885786\n",
            "Epoch 37 avg loss: 0.7284395173192024\n",
            "Epoch 38 avg loss: 0.7284394924839338\n",
            "Epoch 39 avg loss: 0.7284394850333532\n",
            "Epoch 40 avg loss: 0.7284395545721054\n",
            "Epoch 41 avg loss: 0.728439507385095\n",
            "Epoch 42 avg loss: 0.7284394378463427\n",
            "Epoch 43 avg loss: 0.7284396092096964\n",
            "Epoch 44 avg loss: 0.728439542154471\n",
            "Epoch 45 avg loss: 0.7284394676486651\n",
            "Epoch 46 avg loss: 0.7284396042426428\n",
            "Epoch 47 avg loss: 0.7284395645062128\n",
            "Epoch 48 avg loss: 0.7284397234519323\n",
            "Epoch 49 avg loss: 0.7284395347038904\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503896713256836\n",
            "--------------BIG EPOCH 34--------------\n",
            "Epoch 0 avg loss: 0.728439544637998\n",
            "Epoch 1 avg loss: 0.7284396241108576\n",
            "Epoch 2 avg loss: 0.7284396340449651\n",
            "Epoch 3 avg loss: 0.7284395943085352\n",
            "Epoch 4 avg loss: 0.7284394800662994\n",
            "Epoch 5 avg loss: 0.7284394726157188\n",
            "Epoch 6 avg loss: 0.7284396787484487\n",
            "Epoch 7 avg loss: 0.7284395371874174\n",
            "Epoch 8 avg loss: 0.7284395396709442\n",
            "Epoch 9 avg loss: 0.728439599275589\n",
            "Epoch 10 avg loss: 0.7284396290779114\n",
            "Epoch 11 avg loss: 0.7284395645062128\n",
            "Epoch 12 avg loss: 0.7284395024180412\n",
            "Epoch 13 avg loss: 0.7284395371874174\n",
            "Epoch 14 avg loss: 0.7284395024180412\n",
            "Epoch 15 avg loss: 0.7284394825498263\n",
            "Epoch 16 avg loss: 0.728439507385095\n",
            "Epoch 17 avg loss: 0.7284394577145576\n",
            "Epoch 18 avg loss: 0.7284395272533098\n",
            "Epoch 19 avg loss: 0.7284394626816114\n",
            "Epoch 20 avg loss: 0.7284395570556322\n",
            "Epoch 21 avg loss: 0.728439619143804\n",
            "Epoch 22 avg loss: 0.728439581890901\n",
            "Epoch 23 avg loss: 0.7284394900004069\n",
            "Epoch 24 avg loss: 0.728439616660277\n",
            "Epoch 25 avg loss: 0.7284395520885786\n",
            "Epoch 26 avg loss: 0.7284394999345144\n",
            "Epoch 27 avg loss: 0.7284396241108576\n",
            "Epoch 28 avg loss: 0.7284396067261696\n",
            "Epoch 29 avg loss: 0.7284394999345144\n",
            "Epoch 30 avg loss: 0.728439599275589\n",
            "Epoch 31 avg loss: 0.7284395669897398\n",
            "Epoch 32 avg loss: 0.7284395943085352\n",
            "Epoch 33 avg loss: 0.7284395719567934\n",
            "Epoch 34 avg loss: 0.7284396017591158\n",
            "Epoch 35 avg loss: 0.7284396464625994\n",
            "Epoch 36 avg loss: 0.7284395471215248\n",
            "Epoch 37 avg loss: 0.7284396141767502\n",
            "Epoch 38 avg loss: 0.7284395843744278\n",
            "Epoch 39 avg loss: 0.7284394974509875\n",
            "Epoch 40 avg loss: 0.728439395626386\n",
            "Epoch 41 avg loss: 0.7284395198027293\n",
            "Epoch 42 avg loss: 0.7284395570556322\n",
            "Epoch 43 avg loss: 0.7284396514296532\n",
            "Epoch 44 avg loss: 0.7284394974509875\n",
            "Epoch 45 avg loss: 0.7284394726157188\n",
            "Epoch 46 avg loss: 0.7284395049015681\n",
            "Epoch 47 avg loss: 0.7284394800662994\n",
            "Epoch 48 avg loss: 0.7284395347038904\n",
            "Epoch 49 avg loss: 0.728439579407374\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 35--------------\n",
            "Epoch 0 avg loss: 0.7284396141767502\n",
            "Epoch 1 avg loss: 0.7284396688143412\n",
            "Epoch 2 avg loss: 0.7284394850333532\n",
            "Epoch 3 avg loss: 0.7284396390120188\n",
            "Epoch 4 avg loss: 0.72843965391318\n",
            "Epoch 5 avg loss: 0.7284395049015681\n",
            "Epoch 6 avg loss: 0.7284395322203636\n",
            "Epoch 7 avg loss: 0.72843948751688\n",
            "Epoch 8 avg loss: 0.7284395595391592\n",
            "Epoch 9 avg loss: 0.7284396290779114\n",
            "Epoch 10 avg loss: 0.7284394601980845\n",
            "Epoch 11 avg loss: 0.7284396042426428\n",
            "Epoch 12 avg loss: 0.7284395049015681\n",
            "Epoch 13 avg loss: 0.7284395645062128\n",
            "Epoch 14 avg loss: 0.7284395669897398\n",
            "Epoch 15 avg loss: 0.7284394626816114\n",
            "Epoch 16 avg loss: 0.7284396017591158\n",
            "Epoch 17 avg loss: 0.7284396092096964\n",
            "Epoch 18 avg loss: 0.7284395123521487\n",
            "Epoch 19 avg loss: 0.728439507385095\n",
            "Epoch 20 avg loss: 0.7284395669897398\n",
            "Epoch 21 avg loss: 0.7284394477804502\n",
            "Epoch 22 avg loss: 0.7284395148356756\n",
            "Epoch 23 avg loss: 0.7284394378463427\n",
            "Epoch 24 avg loss: 0.7284394378463427\n",
            "Epoch 25 avg loss: 0.7284394850333532\n",
            "Epoch 26 avg loss: 0.7284394204616547\n",
            "Epoch 27 avg loss: 0.7284394626816114\n",
            "Epoch 28 avg loss: 0.7284396067261696\n",
            "Epoch 29 avg loss: 0.7284394651651382\n",
            "Epoch 30 avg loss: 0.7284396216273308\n",
            "Epoch 31 avg loss: 0.7284395769238472\n",
            "Epoch 32 avg loss: 0.7284395893414816\n",
            "Epoch 33 avg loss: 0.7284396290779114\n",
            "Epoch 34 avg loss: 0.7284395570556322\n",
            "Epoch 35 avg loss: 0.7284394378463427\n",
            "Epoch 36 avg loss: 0.728439581890901\n",
            "Epoch 37 avg loss: 0.7284393558899561\n",
            "Epoch 38 avg loss: 0.7284394403298696\n",
            "Epoch 39 avg loss: 0.7284395371874174\n",
            "Epoch 40 avg loss: 0.728439599275589\n",
            "Epoch 41 avg loss: 0.7284394428133965\n",
            "Epoch 42 avg loss: 0.7284396116932234\n",
            "Epoch 43 avg loss: 0.7284396886825562\n",
            "Epoch 44 avg loss: 0.7284395645062128\n",
            "Epoch 45 avg loss: 0.7284394428133965\n",
            "Epoch 46 avg loss: 0.728439524769783\n",
            "Epoch 47 avg loss: 0.72843965391318\n",
            "Epoch 48 avg loss: 0.7284395769238472\n",
            "Epoch 49 avg loss: 0.7284394676486651\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503899097442627\n",
            "--------------BIG EPOCH 36--------------\n",
            "Epoch 0 avg loss: 0.7284394800662994\n",
            "Epoch 1 avg loss: 0.7284395098686218\n",
            "Epoch 2 avg loss: 0.7284394775827726\n",
            "Epoch 3 avg loss: 0.7284395098686218\n",
            "Epoch 4 avg loss: 0.7284393310546875\n",
            "Epoch 5 avg loss: 0.7284395843744278\n",
            "Epoch 6 avg loss: 0.7284395719567934\n",
            "Epoch 7 avg loss: 0.7284395595391592\n",
            "Epoch 8 avg loss: 0.7284396092096964\n",
            "Epoch 9 avg loss: 0.7284394303957621\n",
            "Epoch 10 avg loss: 0.7284395669897398\n",
            "Epoch 11 avg loss: 0.7284396588802338\n",
            "Epoch 12 avg loss: 0.7284395893414816\n",
            "Epoch 13 avg loss: 0.7284395148356756\n",
            "Epoch 14 avg loss: 0.7284394626816114\n",
            "Epoch 15 avg loss: 0.72843965391318\n",
            "Epoch 16 avg loss: 0.7284393707911173\n",
            "Epoch 17 avg loss: 0.7284395843744278\n",
            "Epoch 18 avg loss: 0.7284394949674606\n",
            "Epoch 19 avg loss: 0.7284395297368368\n",
            "Epoch 20 avg loss: 0.7284395520885786\n",
            "Epoch 21 avg loss: 0.7284394601980845\n",
            "Epoch 22 avg loss: 0.7284394179781278\n",
            "Epoch 23 avg loss: 0.7284396340449651\n",
            "Epoch 24 avg loss: 0.7284396414955457\n",
            "Epoch 25 avg loss: 0.7284394378463427\n",
            "Epoch 26 avg loss: 0.728439599275589\n",
            "Epoch 27 avg loss: 0.7284396464625994\n",
            "Epoch 28 avg loss: 0.7284395669897398\n",
            "Epoch 29 avg loss: 0.7284395943085352\n",
            "Epoch 30 avg loss: 0.728439579407374\n",
            "Epoch 31 avg loss: 0.7284394527475039\n",
            "Epoch 32 avg loss: 0.7284394477804502\n",
            "Epoch 33 avg loss: 0.7284395347038904\n",
            "Epoch 34 avg loss: 0.7284396464625994\n",
            "Epoch 35 avg loss: 0.728439544637998\n",
            "Epoch 36 avg loss: 0.7284395024180412\n",
            "Epoch 37 avg loss: 0.7284396936496099\n",
            "Epoch 38 avg loss: 0.7284395272533098\n",
            "Epoch 39 avg loss: 0.7284394999345144\n",
            "Epoch 40 avg loss: 0.7284395769238472\n",
            "Epoch 41 avg loss: 0.7284395893414816\n",
            "Epoch 42 avg loss: 0.7284395123521487\n",
            "Epoch 43 avg loss: 0.7284394850333532\n",
            "Epoch 44 avg loss: 0.7284396265943845\n",
            "Epoch 45 avg loss: 0.7284395645062128\n",
            "Epoch 46 avg loss: 0.7284395322203636\n",
            "Epoch 47 avg loss: 0.7284393707911173\n",
            "Epoch 48 avg loss: 0.728439432879289\n",
            "Epoch 49 avg loss: 0.728439544637998\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503896117210388\n",
            "--------------BIG EPOCH 37--------------\n",
            "Epoch 0 avg loss: 0.728439673781395\n",
            "Epoch 1 avg loss: 0.7284393906593323\n",
            "Epoch 2 avg loss: 0.7284396563967069\n",
            "Epoch 3 avg loss: 0.7284395371874174\n",
            "Epoch 4 avg loss: 0.7284394254287084\n",
            "Epoch 5 avg loss: 0.7284395769238472\n",
            "Epoch 6 avg loss: 0.7284396315614382\n",
            "Epoch 7 avg loss: 0.7284395843744278\n",
            "Epoch 8 avg loss: 0.7284393782416979\n",
            "Epoch 9 avg loss: 0.7284394999345144\n",
            "Epoch 10 avg loss: 0.7284395694732666\n",
            "Epoch 11 avg loss: 0.7284393509229025\n",
            "Epoch 12 avg loss: 0.7284395322203636\n",
            "Epoch 13 avg loss: 0.728439507385095\n",
            "Epoch 14 avg loss: 0.7284395297368368\n",
            "Epoch 15 avg loss: 0.7284395719567934\n",
            "Epoch 16 avg loss: 0.728439619143804\n",
            "Epoch 17 avg loss: 0.7284396241108576\n",
            "Epoch 18 avg loss: 0.7284395123521487\n",
            "Epoch 19 avg loss: 0.7284395843744278\n",
            "Epoch 20 avg loss: 0.7284394999345144\n",
            "Epoch 21 avg loss: 0.7284395520885786\n",
            "Epoch 22 avg loss: 0.7284396489461263\n",
            "Epoch 23 avg loss: 0.728439562022686\n",
            "Epoch 24 avg loss: 0.7284396514296532\n",
            "Epoch 25 avg loss: 0.7284395049015681\n",
            "Epoch 26 avg loss: 0.728439544637998\n",
            "Epoch 27 avg loss: 0.7284395645062128\n",
            "Epoch 28 avg loss: 0.7284394949674606\n",
            "Epoch 29 avg loss: 0.7284395471215248\n",
            "Epoch 30 avg loss: 0.7284395173192024\n",
            "Epoch 31 avg loss: 0.7284395694732666\n",
            "Epoch 32 avg loss: 0.7284395520885786\n",
            "Epoch 33 avg loss: 0.7284394850333532\n",
            "Epoch 34 avg loss: 0.7284396563967069\n",
            "Epoch 35 avg loss: 0.7284394676486651\n",
            "Epoch 36 avg loss: 0.728439542154471\n",
            "Epoch 37 avg loss: 0.7284395371874174\n",
            "Epoch 38 avg loss: 0.7284395893414816\n",
            "Epoch 39 avg loss: 0.7284396812319756\n",
            "Epoch 40 avg loss: 0.7284394775827726\n",
            "Epoch 41 avg loss: 0.7284395918250084\n",
            "Epoch 42 avg loss: 0.7284394626816114\n",
            "Epoch 43 avg loss: 0.7284396116932234\n",
            "Epoch 44 avg loss: 0.7284396514296532\n",
            "Epoch 45 avg loss: 0.7284395123521487\n",
            "Epoch 46 avg loss: 0.7284396067261696\n",
            "Epoch 47 avg loss: 0.7284395868579546\n",
            "Epoch 48 avg loss: 0.7284395222862562\n",
            "Epoch 49 avg loss: 0.7284393633405367\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 38--------------\n",
            "Epoch 0 avg loss: 0.7284393509229025\n",
            "Epoch 1 avg loss: 0.728439542154471\n",
            "Epoch 2 avg loss: 0.7284395148356756\n",
            "Epoch 3 avg loss: 0.7284394726157188\n",
            "Epoch 4 avg loss: 0.7284394577145576\n",
            "Epoch 5 avg loss: 0.7284396067261696\n",
            "Epoch 6 avg loss: 0.7284396464625994\n",
            "Epoch 7 avg loss: 0.728439507385095\n",
            "Epoch 8 avg loss: 0.7284394030769666\n",
            "Epoch 9 avg loss: 0.728439581890901\n",
            "Epoch 10 avg loss: 0.7284394850333532\n",
            "Epoch 11 avg loss: 0.7284394279122353\n",
            "Epoch 12 avg loss: 0.7284395272533098\n",
            "Epoch 13 avg loss: 0.7284395396709442\n",
            "Epoch 14 avg loss: 0.7284396241108576\n",
            "Epoch 15 avg loss: 0.7284395744403204\n",
            "Epoch 16 avg loss: 0.728439562022686\n",
            "Epoch 17 avg loss: 0.7284395396709442\n",
            "Epoch 18 avg loss: 0.7284395098686218\n",
            "Epoch 19 avg loss: 0.7284395347038904\n",
            "Epoch 20 avg loss: 0.728439616660277\n",
            "Epoch 21 avg loss: 0.7284393509229025\n",
            "Epoch 22 avg loss: 0.728439619143804\n",
            "Epoch 23 avg loss: 0.7284397482872009\n",
            "Epoch 24 avg loss: 0.7284395371874174\n",
            "Epoch 25 avg loss: 0.728439544637998\n",
            "Epoch 26 avg loss: 0.7284394279122353\n",
            "Epoch 27 avg loss: 0.7284394279122353\n",
            "Epoch 28 avg loss: 0.7284395471215248\n",
            "Epoch 29 avg loss: 0.728439542154471\n",
            "Epoch 30 avg loss: 0.7284394279122353\n",
            "Epoch 31 avg loss: 0.72843948751688\n",
            "Epoch 32 avg loss: 0.728439636528492\n",
            "Epoch 33 avg loss: 0.7284395595391592\n",
            "Epoch 34 avg loss: 0.7284395545721054\n",
            "Epoch 35 avg loss: 0.7284396315614382\n",
            "Epoch 36 avg loss: 0.7284395893414816\n",
            "Epoch 37 avg loss: 0.7284394179781278\n",
            "Epoch 38 avg loss: 0.7284395148356756\n",
            "Epoch 39 avg loss: 0.7284394825498263\n",
            "Epoch 40 avg loss: 0.7284396017591158\n",
            "Epoch 41 avg loss: 0.7284393558899561\n",
            "Epoch 42 avg loss: 0.728439616660277\n",
            "Epoch 43 avg loss: 0.7284394229451815\n",
            "Epoch 44 avg loss: 0.7284394552310308\n",
            "Epoch 45 avg loss: 0.7284395322203636\n",
            "Epoch 46 avg loss: 0.7284395098686218\n",
            "Epoch 47 avg loss: 0.7284395496050516\n",
            "Epoch 48 avg loss: 0.7284395669897398\n",
            "Epoch 49 avg loss: 0.728439765671889\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.501\n",
            "    Avg loss: 0.6503896713256836\n",
            "--------------BIG EPOCH 39--------------\n",
            "Epoch 0 avg loss: 0.7284395297368368\n",
            "Epoch 1 avg loss: 0.7284396241108576\n",
            "Epoch 2 avg loss: 0.7284395719567934\n",
            "Epoch 3 avg loss: 0.7284396042426428\n",
            "Epoch 4 avg loss: 0.7284395396709442\n",
            "Epoch 5 avg loss: 0.7284395148356756\n",
            "Epoch 6 avg loss: 0.7284393906593323\n",
            "Epoch 7 avg loss: 0.7284394552310308\n",
            "Epoch 8 avg loss: 0.728439544637998\n",
            "Epoch 9 avg loss: 0.7284396414955457\n",
            "Epoch 10 avg loss: 0.7284395347038904\n",
            "Epoch 11 avg loss: 0.7284395496050516\n",
            "Epoch 12 avg loss: 0.7284396439790726\n",
            "Epoch 13 avg loss: 0.7284396067261696\n",
            "Epoch 14 avg loss: 0.7284395024180412\n",
            "Epoch 15 avg loss: 0.7284395297368368\n",
            "Epoch 16 avg loss: 0.7284394949674606\n",
            "Epoch 17 avg loss: 0.728439507385095\n",
            "Epoch 18 avg loss: 0.728439542154471\n",
            "Epoch 19 avg loss: 0.7284395645062128\n",
            "Epoch 20 avg loss: 0.7284394527475039\n",
            "Epoch 21 avg loss: 0.728439562022686\n",
            "Epoch 22 avg loss: 0.7284395595391592\n",
            "Epoch 23 avg loss: 0.7284395645062128\n",
            "Epoch 24 avg loss: 0.728439544637998\n",
            "Epoch 25 avg loss: 0.7284394378463427\n",
            "Epoch 26 avg loss: 0.7284394800662994\n",
            "Epoch 27 avg loss: 0.7284395893414816\n",
            "Epoch 28 avg loss: 0.7284394775827726\n",
            "Epoch 29 avg loss: 0.7284394080440203\n",
            "Epoch 30 avg loss: 0.7284394378463427\n",
            "Epoch 31 avg loss: 0.7284395645062128\n",
            "Epoch 32 avg loss: 0.7284396886825562\n",
            "Epoch 33 avg loss: 0.7284396042426428\n",
            "Epoch 34 avg loss: 0.7284396787484487\n",
            "Epoch 35 avg loss: 0.7284395744403204\n",
            "Epoch 36 avg loss: 0.7284395545721054\n",
            "Epoch 37 avg loss: 0.7284396489461263\n",
            "Epoch 38 avg loss: 0.7284394949674606\n",
            "Epoch 39 avg loss: 0.7284395496050516\n",
            "Epoch 40 avg loss: 0.7284395198027293\n",
            "Epoch 41 avg loss: 0.7284396017591158\n",
            "Epoch 42 avg loss: 0.728439544637998\n",
            "Epoch 43 avg loss: 0.728439562022686\n",
            "Epoch 44 avg loss: 0.7284395719567934\n",
            "Epoch 45 avg loss: 0.7284395024180412\n",
            "Epoch 46 avg loss: 0.7284394229451815\n",
            "Epoch 47 avg loss: 0.7284395148356756\n",
            "Epoch 48 avg loss: 0.728439524769783\n",
            "Epoch 49 avg loss: 0.728439599275589\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503893733024597\n",
            "--------------BIG EPOCH 40--------------\n",
            "Epoch 0 avg loss: 0.7284395545721054\n",
            "Epoch 1 avg loss: 0.7284395744403204\n",
            "Epoch 2 avg loss: 0.7284396464625994\n",
            "Epoch 3 avg loss: 0.7284395024180412\n",
            "Epoch 4 avg loss: 0.7284395496050516\n",
            "Epoch 5 avg loss: 0.7284396017591158\n",
            "Epoch 6 avg loss: 0.7284396439790726\n",
            "Epoch 7 avg loss: 0.728439616660277\n",
            "Epoch 8 avg loss: 0.7284394577145576\n",
            "Epoch 9 avg loss: 0.72843948751688\n",
            "Epoch 10 avg loss: 0.7284394900004069\n",
            "Epoch 11 avg loss: 0.7284394850333532\n",
            "Epoch 12 avg loss: 0.728439524769783\n",
            "Epoch 13 avg loss: 0.7284395173192024\n",
            "Epoch 14 avg loss: 0.728439579407374\n",
            "Epoch 15 avg loss: 0.7284395024180412\n",
            "Epoch 16 avg loss: 0.7284397458036741\n",
            "Epoch 17 avg loss: 0.7284394651651382\n",
            "Epoch 18 avg loss: 0.7284394726157188\n",
            "Epoch 19 avg loss: 0.728439524769783\n",
            "Epoch 20 avg loss: 0.728439619143804\n",
            "Epoch 21 avg loss: 0.7284395272533098\n",
            "Epoch 22 avg loss: 0.7284394626816114\n",
            "Epoch 23 avg loss: 0.728439581890901\n",
            "Epoch 24 avg loss: 0.7284394900004069\n",
            "Epoch 25 avg loss: 0.7284396290779114\n",
            "Epoch 26 avg loss: 0.728439562022686\n",
            "Epoch 27 avg loss: 0.7284395198027293\n",
            "Epoch 28 avg loss: 0.7284394552310308\n",
            "Epoch 29 avg loss: 0.7284395744403204\n",
            "Epoch 30 avg loss: 0.7284396092096964\n",
            "Epoch 31 avg loss: 0.7284394924839338\n",
            "Epoch 32 avg loss: 0.7284395669897398\n",
            "Epoch 33 avg loss: 0.728439616660277\n",
            "Epoch 34 avg loss: 0.728439673781395\n",
            "Epoch 35 avg loss: 0.7284395645062128\n",
            "Epoch 36 avg loss: 0.7284397259354591\n",
            "Epoch 37 avg loss: 0.7284393310546875\n",
            "Epoch 38 avg loss: 0.7284396241108576\n",
            "Epoch 39 avg loss: 0.7284394676486651\n",
            "Epoch 40 avg loss: 0.7284394949674606\n",
            "Epoch 41 avg loss: 0.7284396265943845\n",
            "Epoch 42 avg loss: 0.728439524769783\n",
            "Epoch 43 avg loss: 0.728439542154471\n",
            "Epoch 44 avg loss: 0.7284395769238472\n",
            "Epoch 45 avg loss: 0.7284394303957621\n",
            "Epoch 46 avg loss: 0.7284395744403204\n",
            "Epoch 47 avg loss: 0.7284395347038904\n",
            "Epoch 48 avg loss: 0.7284395943085352\n",
            "Epoch 49 avg loss: 0.7284394403298696\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503893733024597\n",
            "--------------BIG EPOCH 41--------------\n",
            "Epoch 0 avg loss: 0.7284396092096964\n",
            "Epoch 1 avg loss: 0.7284394750992457\n",
            "Epoch 2 avg loss: 0.7284395123521487\n",
            "Epoch 3 avg loss: 0.7284395024180412\n",
            "Epoch 4 avg loss: 0.7284395595391592\n",
            "Epoch 5 avg loss: 0.7284396042426428\n",
            "Epoch 6 avg loss: 0.7284394949674606\n",
            "Epoch 7 avg loss: 0.728439765671889\n",
            "Epoch 8 avg loss: 0.728439581890901\n",
            "Epoch 9 avg loss: 0.7284396092096964\n",
            "Epoch 10 avg loss: 0.7284395049015681\n",
            "Epoch 11 avg loss: 0.7284393732746443\n",
            "Epoch 12 avg loss: 0.7284396265943845\n",
            "Epoch 13 avg loss: 0.7284394974509875\n",
            "Epoch 14 avg loss: 0.7284396762649218\n",
            "Epoch 15 avg loss: 0.7284395222862562\n",
            "Epoch 16 avg loss: 0.7284395496050516\n",
            "Epoch 17 avg loss: 0.7284394750992457\n",
            "Epoch 18 avg loss: 0.7284395645062128\n",
            "Epoch 19 avg loss: 0.7284395769238472\n",
            "Epoch 20 avg loss: 0.7284394800662994\n",
            "Epoch 21 avg loss: 0.7284395918250084\n",
            "Epoch 22 avg loss: 0.7284396017591158\n",
            "Epoch 23 avg loss: 0.728439599275589\n",
            "Epoch 24 avg loss: 0.7284395371874174\n",
            "Epoch 25 avg loss: 0.7284396340449651\n",
            "Epoch 26 avg loss: 0.7284395868579546\n",
            "Epoch 27 avg loss: 0.7284395769238472\n",
            "Epoch 28 avg loss: 0.7284394254287084\n",
            "Epoch 29 avg loss: 0.7284395843744278\n",
            "Epoch 30 avg loss: 0.7284394775827726\n",
            "Epoch 31 avg loss: 0.7284394577145576\n",
            "Epoch 32 avg loss: 0.7284394924839338\n",
            "Epoch 33 avg loss: 0.7284394900004069\n",
            "Epoch 34 avg loss: 0.7284394626816114\n",
            "Epoch 35 avg loss: 0.728439432879289\n",
            "Epoch 36 avg loss: 0.7284394005934397\n",
            "Epoch 37 avg loss: 0.72843965391318\n",
            "Epoch 38 avg loss: 0.728439524769783\n",
            "Epoch 39 avg loss: 0.7284393707911173\n",
            "Epoch 40 avg loss: 0.7284392938017845\n",
            "Epoch 41 avg loss: 0.7284395198027293\n",
            "Epoch 42 avg loss: 0.7284392838676771\n",
            "Epoch 43 avg loss: 0.7284391423066457\n",
            "Epoch 44 avg loss: 0.7284395322203636\n",
            "Epoch 45 avg loss: 0.7284394204616547\n",
            "Epoch 46 avg loss: 0.7284396414955457\n",
            "Epoch 47 avg loss: 0.7284394750992457\n",
            "Epoch 48 avg loss: 0.7284393459558487\n",
            "Epoch 49 avg loss: 0.7284393186370531\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503900289535522\n",
            "--------------BIG EPOCH 42--------------\n",
            "Epoch 0 avg loss: 0.7284398103753725\n",
            "Epoch 1 avg loss: 0.7284396688143412\n",
            "Epoch 2 avg loss: 0.7284397557377815\n",
            "Epoch 3 avg loss: 0.7284395173192024\n",
            "Epoch 4 avg loss: 0.7284392764170965\n",
            "Epoch 5 avg loss: 0.7284396067261696\n",
            "Epoch 6 avg loss: 0.7284396290779114\n",
            "Epoch 7 avg loss: 0.7284395396709442\n",
            "Epoch 8 avg loss: 0.7284394403298696\n",
            "Epoch 9 avg loss: 0.7284396812319756\n",
            "Epoch 10 avg loss: 0.7284394527475039\n",
            "Epoch 11 avg loss: 0.7284396489461263\n",
            "Epoch 12 avg loss: 0.7284394850333532\n",
            "Epoch 13 avg loss: 0.7284394378463427\n",
            "Epoch 14 avg loss: 0.7284394179781278\n",
            "Epoch 15 avg loss: 0.7284394676486651\n",
            "Epoch 16 avg loss: 0.728439673781395\n",
            "Epoch 17 avg loss: 0.7284394552310308\n",
            "Epoch 18 avg loss: 0.7284396613637606\n",
            "Epoch 19 avg loss: 0.7284395471215248\n",
            "Epoch 20 avg loss: 0.7284394229451815\n",
            "Epoch 21 avg loss: 0.728439599275589\n",
            "Epoch 22 avg loss: 0.7284395222862562\n",
            "Epoch 23 avg loss: 0.7284395669897398\n",
            "Epoch 24 avg loss: 0.7284395719567934\n",
            "Epoch 25 avg loss: 0.7284397209684054\n",
            "Epoch 26 avg loss: 0.7284395694732666\n",
            "Epoch 27 avg loss: 0.7284397780895233\n",
            "Epoch 28 avg loss: 0.728439691166083\n",
            "Epoch 29 avg loss: 0.7284395843744278\n",
            "Epoch 30 avg loss: 0.7284395322203636\n",
            "Epoch 31 avg loss: 0.728439562022686\n",
            "Epoch 32 avg loss: 0.7284394676486651\n",
            "Epoch 33 avg loss: 0.7284395669897398\n",
            "Epoch 34 avg loss: 0.7284395669897398\n",
            "Epoch 35 avg loss: 0.7284397085507711\n",
            "Epoch 36 avg loss: 0.7284396688143412\n",
            "Epoch 37 avg loss: 0.7284395471215248\n",
            "Epoch 38 avg loss: 0.7284394055604935\n",
            "Epoch 39 avg loss: 0.7284394900004069\n",
            "Epoch 40 avg loss: 0.7284395222862562\n",
            "Epoch 41 avg loss: 0.7284397358695666\n",
            "Epoch 42 avg loss: 0.7284395272533098\n",
            "Epoch 43 avg loss: 0.7284395843744278\n",
            "Epoch 44 avg loss: 0.7284395570556322\n",
            "Epoch 45 avg loss: 0.7284396017591158\n",
            "Epoch 46 avg loss: 0.7284396712978681\n",
            "Epoch 47 avg loss: 0.7284395347038904\n",
            "Epoch 48 avg loss: 0.7284396067261696\n",
            "Epoch 49 avg loss: 0.7284395098686218\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 43--------------\n",
            "Epoch 0 avg loss: 0.7284394651651382\n",
            "Epoch 1 avg loss: 0.7284396290779114\n",
            "Epoch 2 avg loss: 0.7284395595391592\n",
            "Epoch 3 avg loss: 0.728439542154471\n",
            "Epoch 4 avg loss: 0.7284395545721054\n",
            "Epoch 5 avg loss: 0.7284395520885786\n",
            "Epoch 6 avg loss: 0.7284393782416979\n",
            "Epoch 7 avg loss: 0.7284396067261696\n",
            "Epoch 8 avg loss: 0.7284395123521487\n",
            "Epoch 9 avg loss: 0.7284394527475039\n",
            "Epoch 10 avg loss: 0.728439581890901\n",
            "Epoch 11 avg loss: 0.7284396092096964\n",
            "Epoch 12 avg loss: 0.7284395545721054\n",
            "Epoch 13 avg loss: 0.7284394005934397\n",
            "Epoch 14 avg loss: 0.7284394750992457\n",
            "Epoch 15 avg loss: 0.7284395719567934\n",
            "Epoch 16 avg loss: 0.728439450263977\n",
            "Epoch 17 avg loss: 0.7284395967920622\n",
            "Epoch 18 avg loss: 0.7284394825498263\n",
            "Epoch 19 avg loss: 0.728439599275589\n",
            "Epoch 20 avg loss: 0.7284395371874174\n",
            "Epoch 21 avg loss: 0.7284393633405367\n",
            "Epoch 22 avg loss: 0.7284394651651382\n",
            "Epoch 23 avg loss: 0.728439581890901\n",
            "Epoch 24 avg loss: 0.7284394900004069\n",
            "Epoch 25 avg loss: 0.7284394924839338\n",
            "Epoch 26 avg loss: 0.7284395545721054\n",
            "Epoch 27 avg loss: 0.7284396812319756\n",
            "Epoch 28 avg loss: 0.7284396489461263\n",
            "Epoch 29 avg loss: 0.7284396439790726\n",
            "Epoch 30 avg loss: 0.7284395297368368\n",
            "Epoch 31 avg loss: 0.728439636528492\n",
            "Epoch 32 avg loss: 0.7284395918250084\n",
            "Epoch 33 avg loss: 0.7284396241108576\n",
            "Epoch 34 avg loss: 0.7284396489461263\n",
            "Epoch 35 avg loss: 0.7284394800662994\n",
            "Epoch 36 avg loss: 0.7284394179781278\n",
            "Epoch 37 avg loss: 0.7284395297368368\n",
            "Epoch 38 avg loss: 0.7284395347038904\n",
            "Epoch 39 avg loss: 0.7284397011001905\n",
            "Epoch 40 avg loss: 0.7284395520885786\n",
            "Epoch 41 avg loss: 0.7284395645062128\n",
            "Epoch 42 avg loss: 0.728439507385095\n",
            "Epoch 43 avg loss: 0.7284394378463427\n",
            "Epoch 44 avg loss: 0.7284394924839338\n",
            "Epoch 45 avg loss: 0.7284396216273308\n",
            "Epoch 46 avg loss: 0.72843948751688\n",
            "Epoch 47 avg loss: 0.7284394850333532\n",
            "Epoch 48 avg loss: 0.7284395148356756\n",
            "Epoch 49 avg loss: 0.728439470132192\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503898501396179\n",
            "--------------BIG EPOCH 44--------------\n",
            "Epoch 0 avg loss: 0.7284396241108576\n",
            "Epoch 1 avg loss: 0.728439581890901\n",
            "Epoch 2 avg loss: 0.7284395967920622\n",
            "Epoch 3 avg loss: 0.7284394577145576\n",
            "Epoch 4 avg loss: 0.7284394105275472\n",
            "Epoch 5 avg loss: 0.7284396613637606\n",
            "Epoch 6 avg loss: 0.7284395173192024\n",
            "Epoch 7 avg loss: 0.7284394055604935\n",
            "Epoch 8 avg loss: 0.7284394999345144\n",
            "Epoch 9 avg loss: 0.7284393732746443\n",
            "Epoch 10 avg loss: 0.7284395595391592\n",
            "Epoch 11 avg loss: 0.7284396712978681\n",
            "Epoch 12 avg loss: 0.7284394552310308\n",
            "Epoch 13 avg loss: 0.7284395322203636\n",
            "Epoch 14 avg loss: 0.7284396712978681\n",
            "Epoch 15 avg loss: 0.7284396141767502\n",
            "Epoch 16 avg loss: 0.7284395669897398\n",
            "Epoch 17 avg loss: 0.7284396290779114\n",
            "Epoch 18 avg loss: 0.7284396390120188\n",
            "Epoch 19 avg loss: 0.728439599275589\n",
            "Epoch 20 avg loss: 0.7284395198027293\n",
            "Epoch 21 avg loss: 0.7284396414955457\n",
            "Epoch 22 avg loss: 0.7284394601980845\n",
            "Epoch 23 avg loss: 0.7284395669897398\n",
            "Epoch 24 avg loss: 0.7284394949674606\n",
            "Epoch 25 avg loss: 0.7284395918250084\n",
            "Epoch 26 avg loss: 0.7284394428133965\n",
            "Epoch 27 avg loss: 0.7284397234519323\n",
            "Epoch 28 avg loss: 0.7284395744403204\n",
            "Epoch 29 avg loss: 0.7284394552310308\n",
            "Epoch 30 avg loss: 0.7284395123521487\n",
            "Epoch 31 avg loss: 0.7284393236041069\n",
            "Epoch 32 avg loss: 0.7284394378463427\n",
            "Epoch 33 avg loss: 0.7284394154946009\n",
            "Epoch 34 avg loss: 0.7284394130110741\n",
            "Epoch 35 avg loss: 0.7284394974509875\n",
            "Epoch 36 avg loss: 0.7284395595391592\n",
            "Epoch 37 avg loss: 0.7284394800662994\n",
            "Epoch 38 avg loss: 0.728439691166083\n",
            "Epoch 39 avg loss: 0.7284395496050516\n",
            "Epoch 40 avg loss: 0.7284394800662994\n",
            "Epoch 41 avg loss: 0.7284396712978681\n",
            "Epoch 42 avg loss: 0.728439562022686\n",
            "Epoch 43 avg loss: 0.728439581890901\n",
            "Epoch 44 avg loss: 0.72843965391318\n",
            "Epoch 45 avg loss: 0.7284395371874174\n",
            "Epoch 46 avg loss: 0.7284395371874174\n",
            "Epoch 47 avg loss: 0.7284395893414816\n",
            "Epoch 48 avg loss: 0.7284395272533098\n",
            "Epoch 49 avg loss: 0.728439524769783\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.501\n",
            "    Avg loss: 0.6503896117210388\n",
            "--------------BIG EPOCH 45--------------\n",
            "Epoch 0 avg loss: 0.7284396340449651\n",
            "Epoch 1 avg loss: 0.7284395198027293\n",
            "Epoch 2 avg loss: 0.7284395669897398\n",
            "Epoch 3 avg loss: 0.7284394577145576\n",
            "Epoch 4 avg loss: 0.7284394999345144\n",
            "Epoch 5 avg loss: 0.7284394477804502\n",
            "Epoch 6 avg loss: 0.7284394428133965\n",
            "Epoch 7 avg loss: 0.7284393707911173\n",
            "Epoch 8 avg loss: 0.7284396762649218\n",
            "Epoch 9 avg loss: 0.728439450263977\n",
            "Epoch 10 avg loss: 0.7284394924839338\n",
            "Epoch 11 avg loss: 0.7284395222862562\n",
            "Epoch 12 avg loss: 0.7284394825498263\n",
            "Epoch 13 avg loss: 0.7284396141767502\n",
            "Epoch 14 avg loss: 0.7284394279122353\n",
            "Epoch 15 avg loss: 0.7284396812319756\n",
            "Epoch 16 avg loss: 0.7284397333860397\n",
            "Epoch 17 avg loss: 0.7284395545721054\n",
            "Epoch 18 avg loss: 0.7284394800662994\n",
            "Epoch 19 avg loss: 0.72843948751688\n",
            "Epoch 20 avg loss: 0.7284395719567934\n",
            "Epoch 21 avg loss: 0.7284395049015681\n",
            "Epoch 22 avg loss: 0.7284394651651382\n",
            "Epoch 23 avg loss: 0.7284395545721054\n",
            "Epoch 24 avg loss: 0.7284394974509875\n",
            "Epoch 25 avg loss: 0.7284395496050516\n",
            "Epoch 26 avg loss: 0.7284396067261696\n",
            "Epoch 27 avg loss: 0.7284395322203636\n",
            "Epoch 28 avg loss: 0.7284396762649218\n",
            "Epoch 29 avg loss: 0.7284396837155024\n",
            "Epoch 30 avg loss: 0.7284395148356756\n",
            "Epoch 31 avg loss: 0.7284394477804502\n",
            "Epoch 32 avg loss: 0.7284394800662994\n",
            "Epoch 33 avg loss: 0.7284394974509875\n",
            "Epoch 34 avg loss: 0.7284395520885786\n",
            "Epoch 35 avg loss: 0.7284395719567934\n",
            "Epoch 36 avg loss: 0.7284395570556322\n",
            "Epoch 37 avg loss: 0.7284397532542547\n",
            "Epoch 38 avg loss: 0.7284395943085352\n",
            "Epoch 39 avg loss: 0.7284395694732666\n",
            "Epoch 40 avg loss: 0.7284394080440203\n",
            "Epoch 41 avg loss: 0.7284394105275472\n",
            "Epoch 42 avg loss: 0.7284396340449651\n",
            "Epoch 43 avg loss: 0.7284397160013517\n",
            "Epoch 44 avg loss: 0.7284395943085352\n",
            "Epoch 45 avg loss: 0.7284395645062128\n",
            "Epoch 46 avg loss: 0.7284395719567934\n",
            "Epoch 47 avg loss: 0.7284395496050516\n",
            "Epoch 48 avg loss: 0.7284395098686218\n",
            "Epoch 49 avg loss: 0.7284394850333532\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 46--------------\n",
            "Epoch 0 avg loss: 0.728439507385095\n",
            "Epoch 1 avg loss: 0.7284395943085352\n",
            "Epoch 2 avg loss: 0.728439616660277\n",
            "Epoch 3 avg loss: 0.7284394924839338\n",
            "Epoch 4 avg loss: 0.7284395198027293\n",
            "Epoch 5 avg loss: 0.7284395744403204\n",
            "Epoch 6 avg loss: 0.7284395645062128\n",
            "Epoch 7 avg loss: 0.7284395098686218\n",
            "Epoch 8 avg loss: 0.7284395123521487\n",
            "Epoch 9 avg loss: 0.7284395198027293\n",
            "Epoch 10 avg loss: 0.7284395347038904\n",
            "Epoch 11 avg loss: 0.728439524769783\n",
            "Epoch 12 avg loss: 0.7284394949674606\n",
            "Epoch 13 avg loss: 0.7284394378463427\n",
            "Epoch 14 avg loss: 0.7284397110342979\n",
            "Epoch 15 avg loss: 0.7284395769238472\n",
            "Epoch 16 avg loss: 0.7284394055604935\n",
            "Epoch 17 avg loss: 0.7284395272533098\n",
            "Epoch 18 avg loss: 0.7284395520885786\n",
            "Epoch 19 avg loss: 0.728439507385095\n",
            "Epoch 20 avg loss: 0.7284396464625994\n",
            "Epoch 21 avg loss: 0.7284396116932234\n",
            "Epoch 22 avg loss: 0.7284395322203636\n",
            "Epoch 23 avg loss: 0.7284395570556322\n",
            "Epoch 24 avg loss: 0.7284395123521487\n",
            "Epoch 25 avg loss: 0.728439507385095\n",
            "Epoch 26 avg loss: 0.7284395098686218\n",
            "Epoch 27 avg loss: 0.728439599275589\n",
            "Epoch 28 avg loss: 0.7284396489461263\n",
            "Epoch 29 avg loss: 0.7284395123521487\n",
            "Epoch 30 avg loss: 0.7284394601980845\n",
            "Epoch 31 avg loss: 0.7284395148356756\n",
            "Epoch 32 avg loss: 0.7284396042426428\n",
            "Epoch 33 avg loss: 0.7284395520885786\n",
            "Epoch 34 avg loss: 0.7284394030769666\n",
            "Epoch 35 avg loss: 0.7284395297368368\n",
            "Epoch 36 avg loss: 0.7284394428133965\n",
            "Epoch 37 avg loss: 0.728439581890901\n",
            "Epoch 38 avg loss: 0.7284396017591158\n",
            "Epoch 39 avg loss: 0.728439599275589\n",
            "Epoch 40 avg loss: 0.7284395049015681\n",
            "Epoch 41 avg loss: 0.728439619143804\n",
            "Epoch 42 avg loss: 0.728439599275589\n",
            "Epoch 43 avg loss: 0.728439581890901\n",
            "Epoch 44 avg loss: 0.7284395173192024\n",
            "Epoch 45 avg loss: 0.7284395173192024\n",
            "Epoch 46 avg loss: 0.7284395371874174\n",
            "Epoch 47 avg loss: 0.7284394924839338\n",
            "Epoch 48 avg loss: 0.7284395694732666\n",
            "Epoch 49 avg loss: 0.728439470132192\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503896117210388\n",
            "--------------BIG EPOCH 47--------------\n",
            "Epoch 0 avg loss: 0.7284395049015681\n",
            "Epoch 1 avg loss: 0.7284396265943845\n",
            "Epoch 2 avg loss: 0.7284394949674606\n",
            "Epoch 3 avg loss: 0.7284396414955457\n",
            "Epoch 4 avg loss: 0.72843948751688\n",
            "Epoch 5 avg loss: 0.7284396712978681\n",
            "Epoch 6 avg loss: 0.7284396464625994\n",
            "Epoch 7 avg loss: 0.728439581890901\n",
            "Epoch 8 avg loss: 0.7284395322203636\n",
            "Epoch 9 avg loss: 0.7284395669897398\n",
            "Epoch 10 avg loss: 0.7284396116932234\n",
            "Epoch 11 avg loss: 0.728439562022686\n",
            "Epoch 12 avg loss: 0.7284396216273308\n",
            "Epoch 13 avg loss: 0.728439450263977\n",
            "Epoch 14 avg loss: 0.728439524769783\n",
            "Epoch 15 avg loss: 0.7284395222862562\n",
            "Epoch 16 avg loss: 0.7284395297368368\n",
            "Epoch 17 avg loss: 0.7284395272533098\n",
            "Epoch 18 avg loss: 0.7284394676486651\n",
            "Epoch 19 avg loss: 0.7284394527475039\n",
            "Epoch 20 avg loss: 0.7284397035837173\n",
            "Epoch 21 avg loss: 0.7284395198027293\n",
            "Epoch 22 avg loss: 0.7284396886825562\n",
            "Epoch 23 avg loss: 0.7284396067261696\n",
            "Epoch 24 avg loss: 0.7284396017591158\n",
            "Epoch 25 avg loss: 0.728439581890901\n",
            "Epoch 26 avg loss: 0.7284394428133965\n",
            "Epoch 27 avg loss: 0.7284395694732666\n",
            "Epoch 28 avg loss: 0.7284395719567934\n",
            "Epoch 29 avg loss: 0.7284396017591158\n",
            "Epoch 30 avg loss: 0.7284396067261696\n",
            "Epoch 31 avg loss: 0.7284395595391592\n",
            "Epoch 32 avg loss: 0.7284394999345144\n",
            "Epoch 33 avg loss: 0.7284394105275472\n",
            "Epoch 34 avg loss: 0.7284396017591158\n",
            "Epoch 35 avg loss: 0.7284394353628159\n",
            "Epoch 36 avg loss: 0.7284395967920622\n",
            "Epoch 37 avg loss: 0.728439636528492\n",
            "Epoch 38 avg loss: 0.7284395371874174\n",
            "Epoch 39 avg loss: 0.7284395496050516\n",
            "Epoch 40 avg loss: 0.728439524769783\n",
            "Epoch 41 avg loss: 0.7284394030769666\n",
            "Epoch 42 avg loss: 0.728439581890901\n",
            "Epoch 43 avg loss: 0.7284396141767502\n",
            "Epoch 44 avg loss: 0.7284395198027293\n",
            "Epoch 45 avg loss: 0.7284395570556322\n",
            "Epoch 46 avg loss: 0.7284393981099129\n",
            "Epoch 47 avg loss: 0.7284394999345144\n",
            "Epoch 48 avg loss: 0.7284394651651382\n",
            "Epoch 49 avg loss: 0.7284396787484487\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.499\n",
            "    Avg loss: 0.6503897309303284\n",
            "--------------BIG EPOCH 48--------------\n",
            "Epoch 0 avg loss: 0.7284395843744278\n",
            "Epoch 1 avg loss: 0.7284395595391592\n",
            "Epoch 2 avg loss: 0.7284394800662994\n",
            "Epoch 3 avg loss: 0.7284395943085352\n",
            "Epoch 4 avg loss: 0.7284395744403204\n",
            "Epoch 5 avg loss: 0.7284395868579546\n",
            "Epoch 6 avg loss: 0.7284395570556322\n",
            "Epoch 7 avg loss: 0.7284395297368368\n",
            "Epoch 8 avg loss: 0.7284394477804502\n",
            "Epoch 9 avg loss: 0.7284395496050516\n",
            "Epoch 10 avg loss: 0.7284395222862562\n",
            "Epoch 11 avg loss: 0.7284395744403204\n",
            "Epoch 12 avg loss: 0.7284396464625994\n",
            "Epoch 13 avg loss: 0.7284395396709442\n",
            "Epoch 14 avg loss: 0.7284395744403204\n",
            "Epoch 15 avg loss: 0.7284396067261696\n",
            "Epoch 16 avg loss: 0.7284395694732666\n",
            "Epoch 17 avg loss: 0.7284395918250084\n",
            "Epoch 18 avg loss: 0.728439507385095\n",
            "Epoch 19 avg loss: 0.728439450263977\n",
            "Epoch 20 avg loss: 0.7284395868579546\n",
            "Epoch 21 avg loss: 0.7284395396709442\n",
            "Epoch 22 avg loss: 0.7284395123521487\n",
            "Epoch 23 avg loss: 0.728439507385095\n",
            "Epoch 24 avg loss: 0.7284395272533098\n",
            "Epoch 25 avg loss: 0.7284395918250084\n",
            "Epoch 26 avg loss: 0.7284395322203636\n",
            "Epoch 27 avg loss: 0.7284394974509875\n",
            "Epoch 28 avg loss: 0.7284395694732666\n",
            "Epoch 29 avg loss: 0.7284394775827726\n",
            "Epoch 30 avg loss: 0.7284396414955457\n",
            "Epoch 31 avg loss: 0.7284394999345144\n",
            "Epoch 32 avg loss: 0.7284396092096964\n",
            "Epoch 33 avg loss: 0.7284395471215248\n",
            "Epoch 34 avg loss: 0.7284394999345144\n",
            "Epoch 35 avg loss: 0.7284395868579546\n",
            "Epoch 36 avg loss: 0.7284395396709442\n",
            "Epoch 37 avg loss: 0.7284395272533098\n",
            "Epoch 38 avg loss: 0.7284395545721054\n",
            "Epoch 39 avg loss: 0.728439544637998\n",
            "Epoch 40 avg loss: 0.728439636528492\n",
            "Epoch 41 avg loss: 0.7284395868579546\n",
            "Epoch 42 avg loss: 0.7284395272533098\n",
            "Epoch 43 avg loss: 0.728439562022686\n",
            "Epoch 44 avg loss: 0.7284394800662994\n",
            "Epoch 45 avg loss: 0.7284397011001905\n",
            "Epoch 46 avg loss: 0.728439619143804\n",
            "Epoch 47 avg loss: 0.7284395645062128\n",
            "Epoch 48 avg loss: 0.728439579407374\n",
            "Epoch 49 avg loss: 0.728439616660277\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.500\n",
            "    Avg loss: 0.6503893733024597\n",
            "--------------BIG EPOCH 49--------------\n",
            "Epoch 0 avg loss: 0.7284395347038904\n",
            "Epoch 1 avg loss: 0.728439599275589\n",
            "Epoch 2 avg loss: 0.7284393807252248\n",
            "Epoch 3 avg loss: 0.7284395744403204\n",
            "Epoch 4 avg loss: 0.7284395719567934\n",
            "Epoch 5 avg loss: 0.7284395396709442\n",
            "Epoch 6 avg loss: 0.728439581890901\n",
            "Epoch 7 avg loss: 0.7284394949674606\n",
            "Epoch 8 avg loss: 0.7284395272533098\n",
            "Epoch 9 avg loss: 0.7284394254287084\n",
            "Epoch 10 avg loss: 0.7284394999345144\n",
            "Epoch 11 avg loss: 0.7284395694732666\n",
            "Epoch 12 avg loss: 0.7284395396709442\n",
            "Epoch 13 avg loss: 0.7284395843744278\n",
            "Epoch 14 avg loss: 0.7284395943085352\n",
            "Epoch 15 avg loss: 0.7284395967920622\n",
            "Epoch 16 avg loss: 0.7284395098686218\n",
            "Epoch 17 avg loss: 0.728439599275589\n",
            "Epoch 18 avg loss: 0.7284396067261696\n",
            "Epoch 19 avg loss: 0.728439636528492\n",
            "Epoch 20 avg loss: 0.728439579407374\n",
            "Epoch 21 avg loss: 0.7284395943085352\n",
            "Epoch 22 avg loss: 0.7284394999345144\n",
            "Epoch 23 avg loss: 0.7284396514296532\n",
            "Epoch 24 avg loss: 0.7284394800662994\n",
            "Epoch 25 avg loss: 0.7284394626816114\n",
            "Epoch 26 avg loss: 0.7284394676486651\n",
            "Epoch 27 avg loss: 0.7284394452969233\n",
            "Epoch 28 avg loss: 0.728439432879289\n",
            "Epoch 29 avg loss: 0.7284396588802338\n",
            "Epoch 30 avg loss: 0.7284395148356756\n",
            "Epoch 31 avg loss: 0.7284396812319756\n",
            "Epoch 32 avg loss: 0.7284394080440203\n",
            "Epoch 33 avg loss: 0.7284394775827726\n",
            "Epoch 34 avg loss: 0.7284396290779114\n",
            "Epoch 35 avg loss: 0.7284393360217413\n",
            "Epoch 36 avg loss: 0.7284394452969233\n",
            "Epoch 37 avg loss: 0.7284397060672442\n",
            "Epoch 38 avg loss: 0.7284395893414816\n",
            "Epoch 39 avg loss: 0.7284394601980845\n",
            "Epoch 40 avg loss: 0.7284394800662994\n",
            "Epoch 41 avg loss: 0.728439616660277\n",
            "Epoch 42 avg loss: 0.7284395645062128\n",
            "Epoch 43 avg loss: 0.7284395347038904\n",
            "Epoch 44 avg loss: 0.7284395148356756\n",
            "Epoch 45 avg loss: 0.7284395347038904\n",
            "Epoch 46 avg loss: 0.7284395719567934\n",
            "Epoch 47 avg loss: 0.7284396563967069\n",
            "Epoch 48 avg loss: 0.7284396216273308\n",
            "Epoch 49 avg loss: 0.7284395396709442\n",
            "Test:\n",
            "    AP: 0.345 \n",
            "    ROC: 0.501\n",
            "    Avg loss: 0.6503897905349731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TZaH1wvIEQ_u"
      },
      "id": "TZaH1wvIEQ_u",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ebtk3K9oERCP"
      },
      "id": "Ebtk3K9oERCP",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tttw0a4oERE2"
      },
      "id": "tttw0a4oERE2",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pk_mmmFuERIL"
      },
      "id": "Pk_mmmFuERIL",
      "execution_count": 66,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "SVD embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}